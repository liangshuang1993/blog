主要用于跟踪最新的TTS文章


tacotron2结构可以参见tacotron2一文.这里讲述一下其他的结构.



----------------

#### Neural Speech Synthesis with Transformer Network


这篇文章将transformer用于TTS. 之前的TTS方法包括Tacotron2存在一些问题: training和inference时效率低; 用RNN难以对长依赖建模. 采用transformer结构一方面可以将encoder和decoder的hidden state进行并行化计算, 另一方面通过self-attention可以将任意两个输入在任意时间上都可以联系起来, 可以解决长依赖问题. 本文指出用Transformer TTS**每步训练时间**可以比Tacotron2 速度提升大约**4.25倍**(但是参数量是TTS的两倍, 仍需要约3天收敛, Tacotron2为4.5天), MOS值略有提升.


**模型结构**

**text-to-phoneme**

首先,将英文单词转为了phoneme. 

**scaled positional encoding**

Transformer结构中没有RNN和CNN, 因此即使打乱了encoder或者decoder的输入序列, 我们仍会得到同样的输出, 因此需要将位置信息放入模型中. 

和原始Transformer中的position embedding一样:

$$PE(pos, 2i) = sin(\frac {pos} {1000^{\frac{2i}{d_{model}}}})$$
$$PE(pos, 2i+1) = cos(\frac {pos} {1000^{\frac{2i}{d_{model}}}})$$


在NMT中, source和target language的embedding相同, 但是在TTS中文本维度和mel谱维度并不同, 因此这里加了一个可训练的prenet, 用来更改position embedding的维度.


$$x_i=prenet(phoneme_i) + \alpha PE(i), \alpha $$为可训练的权重

**encoder pre-net**

Tacotron-2中, text embedding首先输入到了3层的CNN网络中,来学习长距离的context. 在本文中, 将phoneme embedding送入了同样的网络, 称为encoder pre-net.  phoneme embedding的维度为512, CNN每层输出为512. CNN后面跟着BN层和ReLU激活曾, dropout层. 最后的ReLU输出后面还要加一个线性层, 因为ReLU的范围为[0, +∞], 而position embedding的范围为[-1, 1]. 

python代码如下, 参考 https://github.com/soobinseo/Transformer-TTS/ 


```python
class EncoderPrenet(nn.Module):
    def __init__(self, embedding_size=512, num_hidden=512):
        super(EncoderPrenet, self).__init__()
        self.embedding_size = embedding_size
        self.embeded = nn.Embedding(len(symbols), embedding_size, padding_idx=0)
        self.conv1 = Conv(in_channels=embedding_size,
                          out_channels=num_hidden,
                          kernel_size=5,
                          padding=int(np.floor(5 / 2)),
                          w_init='relu')
        self.conv2 = Conv(in_channels=num_hidden,
                          out_channels=num_hidden,
                          kernel_size=5,
                          padding=int(np.floor(5 / 2)),
                          w_init='relu')
        self.batch_norm = nn.BatchNorm1d(num_hidden)

        self.dropout = nn.Dropout(p=0.2)
        self.projection = Linear(num_hidden, num_hidden)
    
    def forward(self, inputs):
        inputs = self.embeded(inputs)
        inputs = inputs.transpose(1, 2)
        inputs = self.dropout(self.batch_norm(torch.relu(self.conv1(inputs))))
        for i in range(3 - 1):
            inputs = self.dropout(self.batch_norm(torch.relu(self.conv2(inputs))))
        inputs = inputs.transpose(1, 2)
        inputs = self.projection(inputs)
        return inputs
```


**decoder pre-net**

mel谱首先经过decoder pre-net, 和Tacotron-2一样, 为2层全连接+ReLU(维度为256). 这里解释到phoneme embedding的空间是不固定的,需要训练的, 而mel谱的空间是固定的, pre-net可以将mel谱映射到phoneme embedding相同的空间上, 这样可以计算 <phoneme, mel frame>的相似度, 从而使attention起作用.  在实验中发现维度从256到512并没有带来提升, 反而使得模型更加难以收敛. 最后和encoder-prenet一样, 也加了一层线性映射, 不仅可以使得它中心对称, 还可以使得其与positional embedding具有相同的维度. 


**encoder和decoder**

Tacotron-2中, encoder是BRNN, 这里用Transformer encoder替换. decoder也由原来的2层RNN和location-sensitive attention 替换为了Transformer decoder. 这里用的是dot product based multi-head attention. 论文中提到换成location-sensitive attention很容易超出现存, 并且增大训练时间.

**mel linear, stop linear and post-net**

和Tacotron-2类似, 这里也用了两个不同的线性映射来预测mel谱和stop token, 用5层CNN来产生残差, refine mel谱. 这里提到stopnet训练的时候, 只有句末才有一个正样本, 其他都是负样本,会有很大的样本不均衡. 这里在算loss的时候加了权重(5-8). 


![](/papers/tts/72.png)


**实验**

这里采用的是动态的batch size, 平均batch size约为16. 最开始采用１块GPU进行训练,但是发现训练很不稳定， 产生的音频也不完整， 质量很差， 因此拥有了多GPU训练, 这样可以加大batch size, 之后才解决了这些问题. 

每步训练时间为0.4s, tacotron2则为1.7s, 但是模型是tacotron2的2倍
最终花费3天收敛, tacotron2花费4.5天



----------

#### FastSpeech: Fast, Robust and Controllable Text to Speech

19年5月份刚刚出的论文,基于transformer结构,合成速度可以比transformer-TTS快270倍, 整体合成速度快了38倍, 还可以控制语音速度.

![](/papers/tts/74.png)

模型结构如上图所示,这里没有采用传统的encoder-decoder结构,而是采用了一种全新的feed-forward结构. 主要包含三大块: phoneme边有N个FFT block(基于transformer修改), mel谱边也有N个FFT block,中间连接他们的是length regulator.

**FFT block**

FFT block结构如上图b所示,包含多头自注意力层和conv1D层,和transformer中一样也有residual,layer normalization和dropout.

**length regulator**

length regulator主要是解决了phoneme序列和梅尔谱序列长度不一致的问题,同时也可以控制音频速度和部分韵律. phoneme序列通常比梅尔谱序列短,每个phoneme通常对应几个梅尔谱,我们将一个phoneme对应的梅尔谱长度称为phoneme duration d. 基于d我们可以将phoneme序列的隐状态展开到和梅尔谱相同的长度. 定义phoneme序列的隐状态序列为$H_{pho}=[h_1, h_2, ..., h_n]$, 即n个phoneme, 定义phoneme duration为$D=[d_1, d_2, ..., d_n]$ 其中$\sum_{i=1}^nd_i=m$, m为梅尔谱的长度. 我们将length regulator LR定义为$H_{mel}=LR(H_{pho}, D, \alpha)$, 其中 $\alpha$ 为phoneme展开后的长度$H_{mel}$和目标梅尔谱的长度的比例, 可以用来控制音频速度,目前TTS系统中几乎都没有提到这个变量. 我们也可以在单词之间加入静音来控制停顿.

**duration predictor**

由上面的图d可以看到,duration predictor包含了2层1D CNN,激活函数为ReLU, 每层也接的有layer normalization和dropout, 最后跟上一个线性层. 
这个模块放在phoneme边的FFT block的上面,和FastSpeech模型一起训练.

为了训练这个duration predictor, 我们这里用的是普通的自回归TTS模型中得到的ground-truth phoneme duration, 训练过程如下:
- 首先训练一个自回归的encoder-decoder transformer TTS模型
- 对每个训练样本, 从训练好的模型中提取decoder-to-encoder attention alignments. 由于用的是多头attention, 因此有多个attention, 并不是所有的attention都呈现对角特性. 我们提出了focus rate F来测量attention与对角有多接近: $F=\frac {1}{2} \sum_{s=1}^Smax_{1\leq t\leq T}a_{s,t}$. S和T分别是ground-truth的梅尔谱和phoneme长度, $a_{s,t}$指的是attention矩阵中的元素, 我们对每个head的attention进行计算,用最大的F作为attention alignments.
- 最终根据$d_i=\sum_{s=1}^S1[argmax_ta_{s,t}=i]$(感觉论文中不对)




**实验**

用LJSpeech训练,13100英文, 12500来训练,300来验证,300来测试.将英文转成了音素后才进行的训练.

模型在phoneme侧和梅尔谱侧分别包含4个FFT block, phoneme embedding, self-attention, 1D CNN维度均为384, attention头数为2. 1D CNN的kernel size为3, 输入输出维度为384/1536, 1536/384. 最后的线性层将384维度数据映射到80维. 在duration predictor中, kernel size为3, input/output size为384/384.


自回归transformer TTS模型

这个模型主要有两个作用: 提取phoneme duration, 另外就是在序列层知识蒸馏生成梅尔谱. 这里将原始的transformer block改成了FFT block.


自回归transformer TTS模型训练80K收敛.
在进行知识蒸馏时, 用自回归模型产生梅尔谱, 用文本和合成的梅尔谱作为FastSpeech的训练数据.

将FastSpeech和duration predictor一起训练. 超参和上面相同. 为了加速训练, 我们用自回归transformer TTS中的phoneme embedding, phoneme侧的FFT block初始化. FastSpeech训练了80K步.


结果
![](/papers/tts/75.png)


可以看到FastSpeech比tacotron2和Transformer TTS略低.

![](/papers/tts/76.png)

合成速度则是快了很多.

![](/papers/tts/77.png)

鲁棒性也提高了很多.

另外,FastSpeech还可以更改音频速度, 实现0.5x -1.5x的更改. 还可以通过更改"空格"所对应的duration来修改停顿.