主要用于跟踪最新的TTS文章


tacotron2结构可以参见tacotron2一文.这里讲述一下其他的结构.



----------------

#### Neural Speech Synthesis with Transformer Network


这篇文章将transformer用于TTS. 之前的TTS方法包括Tacotron2存在一些问题: training和inference时效率低; 用RNN难以对长依赖建模. 采用transformer结构一方面可以将encoder和decoder的hidden state进行并行化计算, 另一方面通过self-attention可以将任意两个输入在任意时间上都可以联系起来, 可以解决长依赖问题. 本文指出用Transformer TTS**每步训练时间**可以比Tacotron2 速度提升大约**4.25倍**(但是参数量是TTS的两倍, 仍需要约3天收敛, Tacotron2为4.5天), MOS值略有提升.


**模型结构**

**text-to-phoneme**

首先,将英文单词转为了phoneme. 

**scaled positional encoding**

Transformer结构中没有RNN和CNN, 因此即使打乱了encoder或者decoder的输入序列, 我们仍会得到同样的输出, 因此需要将位置信息放入模型中. 

和原始Transformer中的position embedding一样:

$$PE(pos, 2i) = sin(\frac {pos} {1000^{\frac{2i}{d_{model}}}})$$
$$PE(pos, 2i+1) = cos(\frac {pos} {1000^{\frac{2i}{d_{model}}}})$$


在NMT中, source和target language的embedding相同, 但是在TTS中文本维度和mel谱维度并不同, 因此这里加了一个可训练的prenet, 用来更改position embedding的维度.


$$x_i=prenet(phoneme_i) + \alpha PE(i), \alpha $$为可训练的权重

**encoder pre-net**

Tacotron-2中, text embedding首先输入到了3层的CNN网络中,来学习长距离的context. 在本文中, 将phoneme embedding送入了同样的网络, 称为encoder pre-net.  phoneme embedding的维度为512, CNN每层输出为512. CNN后面跟着BN层和ReLU激活曾, dropout层. 最后的ReLU输出后面还要加一个线性层, 因为ReLU的范围为[0, +∞], 而position embedding的范围为[-1, 1]. 

python代码如下, 参考 https://github.com/soobinseo/Transformer-TTS/ 


```python
class EncoderPrenet(nn.Module):
    def __init__(self, embedding_size=512, num_hidden=512):
        super(EncoderPrenet, self).__init__()
        self.embedding_size = embedding_size
        self.embeded = nn.Embedding(len(symbols), embedding_size, padding_idx=0)
        self.conv1 = Conv(in_channels=embedding_size,
                          out_channels=num_hidden,
                          kernel_size=5,
                          padding=int(np.floor(5 / 2)),
                          w_init='relu')
        self.conv2 = Conv(in_channels=num_hidden,
                          out_channels=num_hidden,
                          kernel_size=5,
                          padding=int(np.floor(5 / 2)),
                          w_init='relu')
        self.batch_norm = nn.BatchNorm1d(num_hidden)

        self.dropout = nn.Dropout(p=0.2)
        self.projection = Linear(num_hidden, num_hidden)
    
    def forward(self, inputs):
        inputs = self.embeded(inputs)
        inputs = inputs.transpose(1, 2)
        inputs = self.dropout(self.batch_norm(torch.relu(self.conv1(inputs))))
        for i in range(3 - 1):
            inputs = self.dropout(self.batch_norm(torch.relu(self.conv2(inputs))))
        inputs = inputs.transpose(1, 2)
        inputs = self.projection(inputs)
        return inputs
```


**decoder pre-net**

mel谱首先经过decoder pre-net, 和Tacotron-2一样, 为2层全连接+ReLU(维度为256). 这里解释到phoneme embedding的空间是不固定的,需要训练的, 而mel谱的空间是固定的, pre-net可以将mel谱映射到phoneme embedding相同的空间上, 这样可以计算 <phoneme, mel frame>的相似度, 从而使attention起作用.  在实验中发现维度从256到512并没有带来提升, 反而使得模型更加难以收敛. 最后和encoder-prenet一样, 也加了一层线性映射, 不仅可以使得它中心对称, 还可以使得其与positional embedding具有相同的维度. 


**encoder和decoder**

Tacotron-2中, encoder是BRNN, 这里用Transformer encoder替换. decoder也由原来的2层RNN和location-sensitive attention 替换为了Transformer decoder. 这里用的是dot product based multi-head attention. 论文中提到换成location-sensitive attention很容易超出现存, 并且增大训练时间.

**mel linear, stop linear and post-net**

和Tacotron-2类似, 这里也用了两个不同的线性映射来预测mel谱和stop token, 用5层CNN来产生残差, refine mel谱. 这里提到stopnet训练的时候, 只有句末才有一个正样本, 其他都是负样本,会有很大的样本不均衡. 这里在算loss的时候加了权重(5-8). 


![](/papers/tts/72.png)


**实验**

每步训练时间为0.4s, tacotron2则为1.7s, 但是模型是tacotron2的2倍
最终花费3天收敛, tacotron2花费4.5天

