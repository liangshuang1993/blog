最近汇总一下waveglow相关的流模型

----

看论文不是很懂, https://kexue.fm/archives/5776 这篇文章讲的比较清楚.


生成模型的本质是要拟合概率分布$p(x)$, 用神经网络得到一个带参数的$q_\theta(x)$, 但是神经网络是一个万能函数拟合器,却不能随意拟合一个概率分布, 因为概率分布有非负和归一化的要求. 刚开始这里我不太明白,因为softmax也可以进行归一化,但是softmax做的归一化其实只能生成离散的分布. 另外深度神经网络还可以生成一些经典的分布的参数从而生成这个分布,比如VAE中生成高斯分布.


严格来说,图像是一个离散的分布,因为它是由有限个像素组成的,而像素的取值是离散的,有限的,因此可以通过离散分布来描述. 这样就是PixelRNN类的模型了.  那么如何研究联系型的分布呢?


我们知道高斯混合分布理论上可以拟合任意分布,那么如果有个高斯分布$q(z)$, 和一个条件高斯分布或者狄拉克分布$q(x|z)$,分布
$$q(x) = \int q(z)q(x|z)dz  \tag {1}$$  
理论上可以拟合任意分布. 对条件概率分布进行参数化$q_\theta(x|z)$, 我们需要求出参数 $\theta$, 利用极大似然, 假设真实数据分布为$\tilde p(x)$, 最大化

$$E_{x \sim{\tilde p(x)}}[logq(x)] \tag{2}$$


不过问题在于$q_{\theta}(x)$是积分形式的,不一定可以算下去.

其他的生成模型如VAE,并没有直接优化上面的期望,而是优化一个更强的上届,这使得它只能是一个近似模型,无法达到良好的生成效果.GAN则是通过一个交替训练的方法绕开了这个困难.

----

flow模型则是直接将积分算出来.

flow模型选择$q(x|z)$为狄拉克分布 $\delta(x - g(z))$,并且$g(z)$ 必须是可逆的,也就是说:

$$x=g(z) \Leftrightarrow z=f(x) \tag{3}$$

如果从理论上实现可逆,那么要求z和x的维度一样. 假设f和g的形式都知道了, 那么通过(1)算q(x)相当于对q(x)做一个积分变换$z=f(x)$, 即本来是:

$$q(x)=\frac{1}{(2\pi)^{D/2}}exp(-\frac{1}{2}||z||^2) \tag{4}$$

的标准高斯分布(D是z的维度). 现在要做一个变换$z=f(x)$ 需要注意变量变换的概率密度函数计算的公式为:

$$p(x) = p(f(x))|det \frac{\partial f}{\partial x}|$$

因此,上面的式子更改为:

$$q(x) = \frac{1}{(2\pi)^{D/2}}exp(-\frac{1}{2}||f(x)||^2) |det \frac{\partial f}{\partial x} |\tag{5}$$

这里对f有两个要求:


1. 可逆, 并且易于求逆函数,它的逆g为我们所希望的生成模型.

2. 对应的雅克比行列式容易计算



两边取log得到

$$log q(x) =-\frac{1}{2}log(2 \pi) - \frac{1}{2}||f(x)||^2) +log |det \frac{\partial f}{\partial x} |\tag{6}$$


这个优化目标是可以求解的,由于f容易求逆, 因此一旦训练完成,我们就可以随机采样一个z,然后通过f的逆来生成一个样本$f^{-1}(z)=g(z)$


---

#### NICE: Non-linear Independent Components Estimation


相对而言,行列式的计算比函数求逆要困难,所以我们要办法满足**行列式好计算**这个要求.

**三角阵的行列式等于对角线元素之积**, 所以我们要想办法使得变换f的雅克比矩阵为三角阵. NICE将D维的x分为两个部分$x_1, x_2$, 然后取下述变换:

$$h_1=x_1$$
$$h_2=x_2+m(x_1)  \tag{7}$$



这个我们称为加性耦合层.

$$\left[\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{x}}\right]=\begin{pmatrix}\mathbb{I}_d & \mathbb{O} \\ 
\left[\frac{\partial \boldsymbol{m}}{\partial \boldsymbol{x}_1}\right] & \mathbb{I}_{d:D}\end{pmatrix}\tag{8}$$

得到的雅克比矩阵是个下三角阵,行列式值为1,对数为0.

并且(7)的变换也是可逆的,逆变换为:

$$x_1=h_1$$
$$x_2=h_2-m(h_1) \tag{9}$$

但是变换(7)中的第一个式子是恒等变换,并不能达到非常强的非线性,所以我们需要多个简单变换的复合,以达到强线性,增强拟合能力.

$$ x = h^{(0)} \leftrightarrow h^{(1)} \leftrightarrow h^{(2)} \leftrightarrow \dots \leftrightarrow h^{(n-1)} \leftrightarrow h^{(n)} = z \tag{10} $$


其中每个变换都是加性耦合层,这样的一个流程称为流(flow).


如果耦合的顺序一直保持不变,那么最后一步的第一部分仍然是$$z_1=x_1$$

为了得到不平凡的变换,我们可以考虑在每次加性耦合前,打乱或者反转输入的各个维度的顺序,或者简单直接交换这两部分的位置,使得信息可以充分混合.


**尺度变换层**


flow是基于可逆变换的,所以当模型训练完之后,我们同时得到了一个生成模型和一个编码模型.但是也因为可逆变换,随机变量z和输入样本x具有同一大小.当我们指定z为高斯分布时,它是遍布整个D维空间的, D也就是输入x的尺寸. 但是虽然x有D维,但是他不一定真的遍布整个D维空间. 也就是说,flow这种基于可逆变换的模型,天生就存在比较严重的维度浪费问题: 输入数据明明都不是D维流形,但是却要编码为一个D维流形.

为了解决这个问题, NICE引入了一个尺度变换层, 它对最后编码出来的每一个维度的特征都做了尺度变换,也就是$z=s \otimes h^{(n)}$, 其中$s=(s_1, s_2, ..., s_D)$也是一个要优化的参数常量(各个元素非负). 这个s向量能识别该维度的重要程度, 越小越重要, 越大说明这个维度越不重要, 起到压缩流形的作用. 注意这个尺度变换层的雅克比行列式就不再是1了,可以算得:

$$\frac{\partial z}{\partial h^{(n)}}=diag(s)$$

它的行列式为$\prod_is_i$, 根据(6), 得到对数似然:

$$logq(x) \sim -\frac{1}{2}||s \otimes f(x)||^2 + \sum_i log s_i \tag{15}$$


我们开始假设z的先验分布为标准正态分布,也就是各个方差都为1, 事实上,我们可以将先验分布的方差也作为训练参数, 这样训练完后方差有大有小, 方差越小,说明该特征的弥散越小,如果方差为0,说明该特征就恒为均值0, 该维度的分布坍塌为一个点,这就意味着流形少了一维.



不同于(4)式, 我们写出带方差的正态分布:

$$q(z)=\frac{1}{(2\pi)^{D/2}\prod^D_{i=1}\sigma_i}exp(-\frac{1}{2}\sum^D_{i=1}\frac{z^2_i}{\sigma^2})$$

将流模型$z=f(x)$代入上式,然后取对数,类似(6)式,我们得到:

$$\log q(x) \sim -\frac{1}{2}\sum_{i=1}^D \frac{f_i^2(x)}{\sigma_i^2} - \sum_{i=1}^D \log \sigma_i\tag{17}$$

对比(15)式,其实就有$s_i=1 / \sigma_i$. 所以尺度变换层等价于将先验部分的方差(标准差)也作为训练参数.


当我们将先验分布选为各分量独立的高斯分布时,除了采样上的方便,还能带来什么好处呢? 在flow模型中有编码器和生成模型,两者是可逆的,所以它和自编码器这种是不同的, 自编码器需要从低维信息来重建高维信息,从而强迫编码器提取有效信息. flow是完全可逆的,不存在信息损失的问题. 那么这个编码器有什么价值呢?

**一个好的特征, 理想情况下各个维度之间应该是相互独立的,这样实现了特征的解耦,使得每个维度都有自己独立的含义**

先验分布为各分量独立的高斯分布,就意味着用f对原始特征进行编码时,输出的编码特征$z=f(x)$的各个维度是解耦的. 在实际生成时,可以控制改变单个的维度,来观察结果.


----

#### Density estimation using real NVP



**仿射耦合层**

$$h_1=x_1 \\
h_2=s(x_1) \otimes x_2 +t(x_1)$$

这里的s, t都是$x_1$的向量函数

