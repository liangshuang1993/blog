最近汇总一下waveglow相关的流模型

----

看论文不是很懂, https://kexue.fm/archives/5776 这篇文章讲的比较清楚.


生成模型的本质是要拟合概率分布$p(x)$, 用神经网络得到一个带参数的$q_\theta(x)$, 但是神经网络是一个万能函数拟合器,却不能随意拟合一个概率分布, 因为概率分布有非负和归一化的要求. 刚开始这里我不太明白,因为softmax也可以进行归一化,但是softmax做的归一化其实只能生成离散的分布. 另外深度神经网络还可以生成一些经典的分布的参数从而生成这个分布,比如VAE中生成高斯分布.


严格来说,图像是一个离散的分布,因为它是由有限个像素组成的,而像素的取值是离散的,有限的,因此可以通过离散分布来描述. 这样就是PixelRNN类的模型了.  那么如何研究联系型的分布呢?


我们知道高斯混合分布理论上可以拟合任意分布,那么如果有个高斯分布$q(z)$, 和一个条件高斯分布或者狄拉克分布$q(x|z)$,分布
$$q(x) = \int q(z)q(x|z)dz  \tag {1}$$  
理论上可以拟合任意分布. 对条件概率分布进行参数化$q_\theta(x|z)$, 我们需要求出参数 $\theta$, 利用极大似然, 假设真实数据分布为$\tilde p(x)$, 最大化

$$E_{x \sim{\tilde p(x)}}[logq(x)] \tag{2}$$


不过问题在于$q_{\theta}(x)$是积分形式的,不一定可以算下去.

其他的生成模型如VAE,并没有直接优化上面的期望,而是优化一个更强的上届,这使得它只能是一个近似模型,无法达到良好的生成效果.GAN则是通过一个交替训练的方法绕开了这个困难.

----

flow模型则是直接将积分算出来.

flow模型选择$q(x|z)$为狄拉克分布 $\delta(x - g(z))$,并且$g(z)$ 必须是可逆的,也就是说:

$$x=g(z) \Leftrightarrow z=f(x) \tag{3}$$

如果从理论上实现可逆,那么要求z和x的维度一样. 假设f和g的形式都知道了, 那么通过(1)算q(x)相当于对q(x)做一个积分变换$z=f(x)$, 即本来是:

$$q(x)=\frac{1}{(2\pi)^{D/2}}exp(-\frac{1}{2}||z||^2) \tag{4}$$

的标准高斯分布(D是z的维度). 现在要做一个变换$z=f(x)$ 需要注意变量变换的概率密度函数计算的公式为:

$$p(x) = p(f(x))|det \frac{\partial f}{\partial x}|$$

因此,上面的式子更改为:

$$q(x) = \frac{1}{(2\pi)^{D/2}}exp(-\frac{1}{2}||f(x)||^2) |det \frac{\partial f}{\partial x} |\tag{5}$$

这里对f有两个要求:


1. 可逆, 并且易于求逆函数,它的逆g为我们所希望的生成模型.

2. 对应的雅克比行列式容易计算



两边取log得到

$$log q(x) =-\frac{1}{2}log(2 \pi) - \frac{1}{2}||f(x)||^2) +log |det \frac{\partial f}{\partial x} |\tag{6}$$


这个优化目标是可以求解的,由于f容易求逆, 因此一旦训练完成,我们就可以随机采样一个z,然后通过f的逆来生成一个样本$f^{-1}(z)=g(z)$


---

#### NICE: Non-linear Independent Components Estimation


相对而言,行列式的计算比函数求逆要困难,所以我们要办法满足**行列式好计算**这个要求.

**三角阵的行列式等于对角线元素之积**, 所以我们要想办法使得变换f的雅克比矩阵为三角阵. NICE将D维的x分为两个部分$x_1, x_2$, 然后取下述变换:

$$h_1=x_1$$
$$h_2=x_2+m(x_1)  \tag{7}$$



这个我们称为加性耦合层.

$$\left[\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{x}}\right]=\begin{pmatrix}\mathbb{I}_d & \mathbb{O} \\ 
\left[\frac{\partial \boldsymbol{m}}{\partial \boldsymbol{x}_1}\right] & \mathbb{I}_{d:D}\end{pmatrix}\tag{8}$$

得到的雅克比矩阵是个下三角阵,行列式值为1,对数为0.

并且(7)的变换也是可逆的,逆变换为:

$$x_1=h_1$$
$$x_2=h_2-m(h_1) \tag{9}$$

但是变换(7)中的第一个式子是恒等变换,并不能达到非常强的非线性,所以我们需要多个简单变换的复合,以达到强线性,增强拟合能力.

$$ x = h^{(0)} \leftrightarrow h^{(1)} \leftrightarrow h^{(2)} \leftrightarrow \dots \leftrightarrow h^{(n-1)} \leftrightarrow h^{(n)} = z \tag{10} $$


其中每个变换都是加性耦合层,这样的一个流程称为流(flow).


如果耦合的顺序一直保持不变,那么最后一步的第一部分仍然是$$z_1=x_1$$

为了得到不平凡的变换,我们可以考虑在每次加性耦合前,打乱或者反转输入的各个维度的顺序,或者简单直接交换这两部分的位置,使得信息可以充分混合.


**尺度变换层**


flow是基于可逆变换的,所以当模型训练完之后,我们同时得到了一个生成模型和一个编码模型.但是也因为可逆变换,随机变量z和输入样本x具有同一大小.当我们指定z为高斯分布时,它是遍布整个D维空间的, D也就是输入x的尺寸. 但是虽然x有D维,但是他不一定真的遍布整个D维空间. 也就是说,flow这种基于可逆变换的模型,天生就存在比较严重的维度浪费问题: 输入数据明明都不是D维流形,但是却要编码为一个D维流形.

为了解决这个问题, NICE引入了一个尺度变换层, 它对最后编码出来的每一个维度的特征都做了尺度变换,也就是$z=s \otimes h^{(n)}$, 其中$s=(s_1, s_2, ..., s_D)$也是一个要优化的参数常量(各个元素非负). 这个s向量能识别该维度的重要程度, 越小越重要, 越大说明这个维度越不重要, 起到压缩流形的作用. 注意这个尺度变换层的雅克比行列式就不再是1了,可以算得:

$$\frac{\partial z}{\partial h^{(n)}}=diag(s)$$

它的行列式为$\prod_is_i$, 根据(6), 得到对数似然:

$$logq(x) \sim -\frac{1}{2}||s \otimes f(x)||^2 + \sum_i log s_i \tag{15}$$


我们开始假设z的先验分布为标准正态分布,也就是各个方差都为1, 事实上,我们可以将先验分布的方差也作为训练参数, 这样训练完后方差有大有小, 方差越小,说明该特征的弥散越小,如果方差为0,说明该特征就恒为均值0, 该维度的分布坍塌为一个点,这就意味着流形少了一维.



不同于(4)式, 我们写出带方差的正态分布:

$$q(z)=\frac{1}{(2\pi)^{D/2}\prod^D_{i=1}\sigma_i}exp(-\frac{1}{2}\sum^D_{i=1}\frac{z^2_i}{\sigma^2})$$

将流模型$z=f(x)$代入上式,然后取对数,类似(6)式,我们得到:

$$\log q(x) \sim -\frac{1}{2}\sum_{i=1}^D \frac{f_i^2(x)}{\sigma_i^2} - \sum_{i=1}^D \log \sigma_i\tag{17}$$

对比(15)式,其实就有$s_i=1 / \sigma_i$. 所以尺度变换层等价于将先验部分的方差(标准差)也作为训练参数.


当我们将先验分布选为各分量独立的高斯分布时,除了采样上的方便,还能带来什么好处呢? 在flow模型中有编码器和生成模型,两者是可逆的,所以它和自编码器这种是不同的, 自编码器需要从低维信息来重建高维信息,从而强迫编码器提取有效信息. flow是完全可逆的,不存在信息损失的问题. 那么这个编码器有什么价值呢?

**一个好的特征, 理想情况下各个维度之间应该是相互独立的,这样实现了特征的解耦,使得每个维度都有自己独立的含义**

先验分布为各分量独立的高斯分布,就意味着用f对原始特征进行编码时,输出的编码特征$z=f(x)$的各个维度是解耦的. 在实际生成时,可以控制改变单个的维度,来观察结果.


----

#### Density estimation using real NVP



**仿射耦合层**

$$h_1=x_1 \\
y_2 = x_2 \otimes exp(s(x_1)) + t(x_1) \tag{1}$$

s指的是scale, t指的是translation. 

得到的雅克比矩阵为

$$\left[\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{x}}\right]=\begin{pmatrix}\mathbb{I}_d & \mathbb{O} \\ 
\left[\frac{\partial y_2}{\partial x_1}\right] & diag(exp[s(x_1)])\end{pmatrix}\tag{2}$$

**masked convolution**

采用两种partioning来利用local correlation: spatial checkerboard和channel-wise masking. 

主要是利用卷积神经网络可以更好的处理图像问题, 减少参数量,发挥并行性能. 利用卷积的前提是输入在空间维度上具有局部相关性, 但是flow中有两个操作: 1. 将输入分成两部分,然后输入到耦合层中 2. 特征输入耦合层之前,要随机打乱原先特征的各个维度. 这两个操作都会破坏局部相关性. 所以如果要利用卷积, 需要保留局部相关性. 图片的三维分别是高度,宽度和channel, 只有channel实际上不具备局部相关性, 因此 RealNVP约定**分割和打乱操作,都只对channel轴执行**.


不过还有一种分割是高度和宽段上进行checkerboard, 这中分割也可以保留空间局部相关性, 不过这种棋盘式的mask相对复杂, 没有什么明显的提升, 所以在glow中已经被抛弃.




**combining**

NICE中是用交错的方式来混合信息流, 这篇论文中则是通过随机的方式将向量打乱, 这样可以使得信息混合更加充分. 


**多尺度结构**

和卷积层一样,这也是一个既减少了模型复杂度,又提升了结果的策略. 


将原始图片由$s \times s \times c$ 变成 $ \frac {c}{2} \times \frac {c}{2} \times 4c$ 

![](/papers/flow/1.png)


-----

#### glow

本文开头指出了flow-based model与GAN和VAE相比的优点:

- flow-based model可以进行确切的隐变量的inference和log-likelihood evaluation. 在VAE中, 只能对和某个数据点对应的隐变量进行近似推断. GAN则根本没有encoder来推断出隐变量. 在可逆的生成模型中,可以不需要近似的进行推断. 这就使得不仅可以进行accurate inference, 还可以对准确的数据的log-likelihood进行优化,而不是优化某个下限.


- efficient inference and efficient synthesis. 自回归模型,比如Pixel-CNN也是可逆的,但是这些模型很难进行并行. 

- 可以为下游任务生成有用的隐空间. 自回归模型的隐层含有未知的边缘分布, 这就使得很难对数据进行控制. 在GAN中, 数据不能用隐空间中的变量直接表示. 可逆的生成模型和VAE是可以的.

- memory saving. 在可逆网络中计算梯度需要的内存是常数,而不是和深度相比是线性的.


这篇文章提出了一种generative flow. 

![](/papers/flow/2.png)

**actnorm**

在RNVP中,采用batch normalization来减轻层数比较深遇到的问题. 但是BN带来的噪声的方差,和每个GPU上的minibatch size成反比, 在小的batch size上面性能会下降. 在某些情况下,可能要使用batch size=1, 因此这篇文章提出actnorm layer(activation normalization).


**可逆的1x1卷积**

RNVP中提出了channel维度上的置换,在这里,采用的是可逆的1x1卷积, 输入维度和输出维度相同. 

对矩阵中向量的置换操作,可以用矩阵乘法来描述.

$$\begin{pmatrix}2 \\ 1 \\ 4 \\ 3\end{pmatrix} = \begin{pmatrix}0 & 1 & 0 & 0\\ 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 1 & 0\end{pmatrix} \begin{pmatrix}1 \\ 2 \\ 3 \\ 4\end{pmatrix}\tag{3}$$

所以其实可以将置换矩阵也换成一个可以训练参数的矩阵. 如果直接用h=xW, 那么它就只是一个没有bias的全连接网络, 并不一定满足可逆以及雅克比行列式好计算这两个要求.所以需要做下面一些操作. 首先就是上面提到的输入维度和输出维度一样, 另外这个变换的雅克比矩阵是W,所以行列式是det W, 所以要把-log |det W| 加入到loss中, 最后, 为了保证W的可逆性, 一般用随机正交矩阵初始化.


对矩阵W进行LU分解,可以将行列式计算的复杂度由$O(c^3)$减少为$O(c)$

$$W=PL(U+diag(s)) \tag(4)$$

L 是下三角矩阵, 对角线为1, U是上三角矩阵, 对角线为0, s是一个向量. 那么,

$$log|det(W)| = sum(log|s|) \tag{5}$$

所以参数首先随机采样得到一个旋转矩阵W, 字后计算它相应的P(固定)和初始化的L和U(这两个需要进行优化)

-----

waveglow

