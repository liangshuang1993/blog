最近整理下流行的几种神经声码器,首先从wavenet开始

#### WaveNet


wavenet是基于PixelCNN的一种生成网络. 声波$x = \{x_1,...,x_T\}$可以用条件概率进行建模:

$$p(x) = \prod^T_{t=1}p(x_t|x_1,...,x_{t-1})$$

每个时刻的声音样本$x_t$是以前面时刻的样本做条件的.

和PixelCNN类似, 这里的条件概率是用一堆CNN层进行建模的. 网络中没有赤化层, 模型的输出和输入的维度相同. 模型输出的是分类的分布,最大化log-likelihood进行优化.


**dilated causal convolutions**


wavenet的主要组成部分为因果卷积, causal convolution. 通过利用因果卷积,我们可以保证模型是以正确的顺序建模,时刻t的音频不能依赖t时刻之后的音频. 

在训练的时候,可以并行计算条件概率,因为所有时刻的ground truth都是已知的. 但是再做inference的时候,不能并行,因为要依赖前面时刻的计算结果.

由于模型中没有recurrent连接, 因此要比RNN训练更快. 不过因果卷积的缺点在于它需要很多层或者较大的kernel size来增大感受野, (=#layers + filter length - 1). 这里我们利用空洞卷积来增大感受野. 如下图所示

![](/papers/vocoder/1.png)

这篇文章中的dilation每层都double, 到达上限后重新再来, 1, 2, 4, ..., 512, 1, 2, 4, ..., 512, 1, 2, 4, .., 512

**softmax distribution**

一种对$p(x_t|x_1,...,x_{t-1})$进行建模的方式是用mixture model如mixture density network, 或者mixture of conditional Gaussian scale mixtures. 但是PixelRNN中指出利用softmax建立离散的像素值,而不是在连续的像素值上面用mixture density方法可以得到更好的结果, 一种原因是类别分布更加灵活,更容易建立任意分布模型,因为对他们的形状没有假设.

原始音频都是用16位的值,一个softmax需要输出65536个类别, 为了使得这个tractable, 我们首先用了μ-law companding transformation, 然后量化到256个值:

$f(x_t) = sign(x_t)\frac{ln(1+\mu|x_t|)}{ln(1+\mu)}$

其中$-1 < x_t < 1, \mu = 255$

这种非线性的量化比直接简单的线性量化重构效果更好. 尤其对于语音而言,我们发现量化后重构效果和原始音频非常相似.

**gated activation units**

用到了和PixelCNN中一样的gated activation单元(这个很多篇论文都有用到):

$$z = tanh(W_{f,k} * x) \odot \sigma(W_{g,k} * x)$$

*代表卷积操作, $\odot$代表元素乘, $\sigma$代表sigmoid, k是层的index, f和g代表filter和gate, W是卷积kernel, 这个激活函数要比relu效果更好.

**residual and skip connection**

![](/papers/vocoder/2.png)


**conditional wavenet**

条件概率分布$p(x|h)=\prod^T_{t=1}p(x_t|x_1,...,x_{t-1}, h)$


在多说话人情况下,我们可以把speaker id送入模型作为额外的输入, 对于TTS应用, 我们可以将文本相关的信息作为额外的输入.

我们把condition分为两类: global condition和local condition, 前者用一个单独的隐变量h表示, 如speaker embedding,可以将上面介绍的激活函数改为:

$$z = tanh(W_{f,k} * x + V^T_{f,k}h) \odot \sigma(W_{g,k} * x + V^T_{g,k}h)$$

$V_{*,k}$是线性变换, $V^T_{*, k}h$在所有时间步上扩展.

针对local condition, 我们引入时间序列$h_t$, 通常比音频信号的采样率更低, 如TTS中的linguistic feature. 我们首先对时间序列做反卷积,映射到新的时间序列$y=f(h)$和原始的音频分辨率相同, 之后送入卷积单元.

$$z = tanh(W_{f,k} * x + V_{f,k} * y) \odot \sigma(W_{g,k} * x + V_{g,k} * y)$$

这里的$V_{f,k} * y$指的是1x1的卷积.另一种替代反卷积网络的方式是用$V_{f,k} * h$然后在时间维度上重复这些值.我们发现这种操作在试验中效果稍差.


**context stacks**

我们已经讨论过了增大感受野的方法: 增大dilation stage的数量, 用更多的层, 更大的滤波器, 更大的dilation factor. 一种补充的方式是用一个单独的,较小的context stack, 处理一长段音频并且locally condition 一个较大的处理一小段音频的wavenet.

**实验**

做了三种任务:
- 多说话人speech generation(没有将文本作为condition)
- TTS
- 音乐合成

