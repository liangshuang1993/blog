### Question Answering

 常用的数据集是SQuAD

 __Standford Attentive Reader: simple neural QA network__

 representation of question: word vector --> BiLSTM --> concate end state

用attention来判断问题的答案在passage的哪部分

predict start token
![](http://latex.codecogs.com/gif.latex?a_i=softmax\limits_i(q^TW_sp_i))

predict end token
![](http://latex.codecogs.com/gif.latex?a_i'=softmax\limits_i(q^TW_s'p_i'))

 __Standford Attentive Reader++__

 - 用LSTM所有的state而不是最后的state得到question representation
 - deep BiLSTM（3 layer）
 - p: vector representation of each token in passage made from concatenation of:
    - word embedding
    - linguistic features
    - term frequency
    - exact match: whether the word appears in the question
    - aligned question embehdding('car' vs 'vehicle')

_最近很多NLP都用到了character level embedding_
__attention flow layer__
attention should from from both way--from the context to the question and from the question to the context

make similarity matrix
![](http://latex.codecogs.com/gif.latex?S_{ij}=w^T_{sim}[c_i;q_j;c_iq_j]) (product)
