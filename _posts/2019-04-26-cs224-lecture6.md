### Language model and RNNs

**language model**

    task of predicting what word comes next.

![](http://latex.codecogs.com/gif.latex?P(x^{x+1}|x^t,...,x^1))

**1. n-gram language model**

- unigrams
- bigrams
- trigrams
- 4-grams

为了构建n-grams模型，需要假设$x^{t+1}$只依赖于前面的n-1个单词。

如何计算这些这些概率呢？  

*counting*

比如studentes opened their ___

$$P(w|students opened their) = \frac{count(students opend their w)}{count(students opend their)}$$

但是它存在一些问题：
- 如果我们不去利用上下文信息，可能会预测错误。
- 分子如果是0，可以增加一个小的delta，smoothing。
- 分母如果是0，减小n。
- storage problem:随着n增加， 模型大小会增加

n比较大的时候，这种sparsity问题比较常见，因此，n通常不大于5.概率较少的时候有时候也是sparisity问题。

**2.a fixed-window neural language model**

和NER类似，假设n=4，先从embedding table中得到$e^1, e^2, e^3, e^4$，之后将其concat起来，得到$e = [e^1, e^2, e^3, e^4]$,

经过一层hidden layer，得到$h=f(We + b_1)$
最后得到$y=softmax(Uh+b_2)$

优点：
- 没有sparsity problem
- 不需要存储所有已经观测到的n-grams

缺点：
- fixed windows太小了
- 不需要存储所有已观测到的n-grams
- n is never large enough
- $x^1, x^2$乘的是不同的参数，不够高效。

**3.RNN来创建language model**

![RNN](/courses/cs224/images/lecture6/1.PNG)







可以用已经训好的embedding，也可以再训练。

优点：
- 可以处理任何长度的输入
- 理论上来说t时刻可以利用前面很多步的信息。
- 对于较长的输入，model size不会增加
- 每个时间步用的是相同的权重。更加高效。

缺点：
- recurrent computation比较慢。不能并行。
- 实际上，很难利用很多步之前的信息。


loss function:
![RNN](/courses/cs224/images/lecture6/2.PNG)

每个时间步都需要计算预测出的logits，并且计算loss，所有的时间步的loss加起来是final loss。

通常由于corpus太大，计算每句话的loss，更新权重。


如何计算梯度呢？
![](/courses/cs224/images/lecture6/3.PNG)

这里利用了multivariable chain rule

![](/courses/cs224/images/lecture6/4.PNG)



n-gram model和RNN language model都可以通过repeated sampling来生成文本


-----
Evaluating language models

![](/courses/cs224/images/lecture6/5.PNG)

perplexity越小越好

----
### text generation有很多应用

![application](/courses/cs224/images/lecture6/6.PNG)




----
RNN也有很多应用，比如用来做sentence classification


![](/courses/cs224/images/lecture6/7.PNG)


可以用来做encoder module

可以做speech recognition，input audio做**condition**

可以做machine language，也是conditional language model





------
想法：
TTS的text embedding
TTS的pronounce model（类似language model，需要看下ASR中language model的应用）
