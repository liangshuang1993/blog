### GAN papers


最近希望将GAN用于TTS上,所以学习一些GAN相关的文章.


#### WGAN

参考文章 https://zhuanlan.zhihu.com/p/25071913

GAN在训练时候有很多问题,比如难以收敛,模式坍塌等,WGAN与原始的GAN相比具有以下优点:

- 彻底解决了GAN训练不稳定的问题, 不需要再小心平衡生成器和判别器的训练程度
- 基本解决了模式坍塌, 确保生成样本的多样性
- 训练过程中终于有一个像交叉熵,准确率这样的数值来指示训练的进程,这个数值越小代表GAN训练的越好,代表生成器生成的图像质量越高.
- 以上一切好处不需要精心设计的网络架构,最简单的多层全连接即可做到

改进后的GAN相比原始的GAN只有四点改进:
- 判别器最后一层去掉sigmoid
- 生成器和判别器的loss不取log
- 每次更新判别器的参数之后把他们的绝对值截断到不超过固定常数c
- 不要用基于动量的优化算法(包括momentum和Adam), 推荐使用RMSProp, SGD也行


算法如下:

learning rate $\alpha = 0.00005$
clipping parameter c = 0.01
batch size m = 64
每个生成器迭代对应的critic迭代次数 $n_{critic} = 5$

![](/papers/gans/7.png)


**part1: 原始GAN存在的问题**

原始GAN中判别器要最小化的损失函数为:

$$-E_{x\sim P_r}[logD(x)] - E_{x \sim P_g}[log(1 - D(x))]$$


一个具体的样本x, 它即可能来自于真实样本也可能来自于生成的样本,它对于损失函数的贡献为:
$$-P_rlogD(x) - P_g(x)log[1 - D(x)]$$

另其关于D(x)的导数为0, 得到:

$$-\frac {P_r(x)}{D(x)}+\frac {P_g(x)}{1-D(x)}$$

得到D的最优解:

$$D^*(x) = \frac{P_r(x)}{P_r(x)+P_g(x)}$$

即样本x来自真实分布和生成分布的可能性的比例.

然而训练GAN有一个trick,就是别把判别器训练的太好,否则在实验过程中生成器完全学不懂(loss降不下去).

Ian原始论文中给出了生成器另一种loss形式,在G效果较差的时候, D很容易就分辨出来真实数据和生成数据,在这种情况下$log(1-D(x))$容易饱和, 因此没有最小化$log(1-D(x))$,而是最大化$E_{x\sim P_g}[log(D(x))]$, 将第一个式子加上一个不依赖生成器的项,得到:

$$E_{x\sim P_r}[logD(x)] + E_{x\sim P_g}[log(1-D(x))]$$

其实就是GAN的优化函数了,将求得的D的最优解带入, 得到:


$$E_{x \sim P_r}log{\frac {P_r(x)}{\frac{1}{2}[P_r(x) + P_g(x)]}} + E_{x \sim P_g}log{\frac {P_g(x)}{\frac{1}{2}[P_r(x) + P_g(x)]}}  - 2log2$$

这样就引入了KL散度和JS散度:

$$KL(P_1||P_2)=E_{x\sim P_1}log{\frac{P_1}{P_2}}$$
$$JS(P_1||P_2)=\frac{1}{2}KL(P_1||\frac{P_1+P_2}{2})+\frac{1}{2}KL(P_2||\frac{P_1+P_2}{2})$$

上面得到的式子可以写成$2JS(P_r||P_g) - 2log2$

在最优判别器下,我们可以把原始GAN的loss转换为最小化真实分布和生成分布之间的JS散度.

问题就出在这个JS散度上, 我们希望两个分布之间越接近,他们的JS散度越小, 我们通过优化JS散度就能将$P_g$拉向$P_r$,这个希望在两个分布有所重叠的时候是成立的,但是如果两个分布完全没有重叠的部分,或者重叠的部分可忽略,它们的JS散度则固定为log2,原因如下:

对于任意一个x,只有以下四种可能:

- $P_1(x)=0$且$P_2(x)=0$
- $P_1(x)\neq0$且$P_2(x)\neq0$
- $P_1(x)=0$且$P_2(x)\neq0$
- $P_1(x)\neq0$且$P_2(x)=0$


第一种对计算JS散度无贡献,第二种重叠部分为可忽略所以贡献也为0,第三种第四种对于式子贡献为log2.

所以当两个分布没有重叠或者重叠部分可忽略时,JS散度固定为log2, 而这意味着**梯度为0**.

那么$P_r$和$P_g$不重叠或者重叠部分可忽略的可能性有多大? 不严谨的答案是非常大.比较严谨的答案是:当$P_r$和$P_g$的支撑集是高维空间中的低维流行时,$P_r$和$P_g$重叠部分测度为0的概率为1.

名词介绍:
- 支撑集(support): 函数的非零部分子集, 一个概率分布的支撑集就是所有概率密度非零部分的集合
- 测度(measure): 高维空间中长度,面积,体积概念的拓广,可以理解为超体积.

GAN中的生成器一般是从低维如100的随机分布采样到一个向量,再经过生成器得到一个高维样本.当生成器的参数固定时, 虽然生成器的概率分布是定义在高维空间上,但是其实是由低维空间的随机分布决定的,所以它的本质维度还是低维的, 再考虑神经网络带来的映射降维,最终可能更小,所以生成样本分布的支撑集就在高维空间如4096上构成一个最多100维的低维流形,撑不满整个高维空间.

撑不满就会导致真实分布与生成分布难以碰到面,. 从低维空间拓展到高维空间时,因为一开始是随机初始化,因此$P_g$和$P_r$几乎没什么关联,所以他们的支撑集之间的重叠部分要么不存在,要么就是比它们的维度还要低至少一个维度,因此测度为0.

> 所以在(近似)最优判别器下,最小化生成器的loss等价于最小化$P_g, P_r$之间的JS散度, 而由于它们两个几乎不可能有不可忽略的重叠,所以无论它们相距多远JS散度都是常数log2, 最终导致生成器的梯度(近似)为0, 梯度消失.

前文提到,Ian论文中另一种形式的loss为$$E_{x\sim P_g}[-log(D(x))]$$.

且

$$KL(P_g||P_r)=E_{x \sim Pg}[log \frac {P_g(x)}{P_r(x)}]=E_{x \sim Pg}[log \frac{1-D^*(x)}{D^*(x)}] $$

可以得到

$$E_{x\sim P_g}[-log(D^*(x))]=KL(P_g||P_r)-E_{x\sim P_g}[-log(1-D^*(x))]$$

最终可以得到,最小化这个loss等价于最小化

$$KL(P_g|P_r)-2JS(P_r|P_g)$$


这个目标函数有两个严重的问题:

- 同时要最小化KL散度,最大化JS散度,在数值上会导致梯度不稳定
- 前面的KL散度不是一个对称的衡量, $KL(P_g|P_r)$与$KL(P_r|P_g)$是有差别的.分析可知容易造成模式坍塌

**part2: WGAN之前的一个过渡解决方案**

原始GAN问题的根本可以归结为两点,一个是等价优化的距离衡量不合理,二是生成器随机初始化后的生成分布很难与真实分布有不可忽略的重叠.

WGAN的前作针对第二点提出了一个解决方案, 就是对生成样本和真实样本加噪声.

**part3: Wasserstein距离**

Wasserstein距离又叫做Earth-Mover(EM)距离,定义如下:

$$W(P_r, P_g)=inf_{\gamma \sim \prod(P_r, P_g)}E_{(x,y)\sim \gamma}[||x-y||]$$


Wasserstein距离相比KL散度,JS散度的优越性在于即使两个分布没有重叠,Wasserstein距离仍能够反映他们的远近.


不过没有办法把W距离直接转换为loss, inf没办法直接求解, 所以利用一个定理把它转成下面形式:

$$W(P_r, P_g) = \frac {1}{K}sup_{||f||_L\leq K}E_{x\sim P_r}[f(x)]-E_{x \sim P_g}[f(x)]$$

其中$$||f||_L$$代表f的Lipschitz常数. 在网络中将神经网络的参数的范围差值不超过某个范围[-c, c], 此时关于输入样本x的倒数也不会超过某个范围, 所以K得以存在.

上面式子的含义是在满足条件下,对所有f取上界,然后除以K, 我们可以用一组参数w来定义一系列可能的函数f, 此时公式变为:

$$K \cdot W(P_r, P_g) \approx max_{w:|f_w|_L \leq K}E_{x \sim P_r}[f_w(x)]-E_{x \sim P_g}[f_w(x)]$$

得到WGAN的loss function

$$-E_{x \sim P_g}[f_w(x)]$$
$$E_{x \sim P_g}[f_w(x)]-E_{x \sim P_r}[f_w(x)]$$


-----

#### Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets

这篇文章将GAN应用到了机器翻译中,这里用的是conditional sequence generative adversarial net.

和TTS类似,常见的机器翻译模型也是一个含有attention的seq2seq模型,但是这些模型通常是优化每个时间步ground word的极大似然估计,这会带来一些问题:很多时候当前的最优并不代表整个句子层面的最优,虽然后面也有一个sentence-level BLEU的提出,但是这种n-gram预测也不能保证好的结果.

本文的模型包含两个方面:一个generator基于输入的source language sentence来生成target language sentence;一个discriminator,condition on source language sentence,预测target language sentence是否是人类生成的. 除了生成想要的分布外,我们还希望可以用一个静态的特定的目标函数直接引导generator,比如生成高的BLEU分.这里使用的是smoothed sentence-level BLEU.训练的时候,用动态的discriminator和静态的BLEU目标函数来引导训练.

**模型结构: BLEU reinforced conditional sequence generative adversarial net**

可以将句子生成的过程视为根据generator产生的策略做选择的过程.采用了policy gradient.

**generator**

和NMT的模型完全一样,这里采用了RNNSearch和Transformer两种

**discriminator**
这里采用CNN结构.由于G产生的句子是不定长的,所以需要用padding.给定source language sequence $X_{1:T}=x_1; ...;x_T$, target language sequence $Y_{1:T}=y_1; ...; y_T$,其中$x_t,y_t \in R^k$.对于输入的矩阵$X_{1:T}$,用$w_j \in R^{l\times k}$记性卷积操作.得到feature map: $c_{ji}=\rho(BN(w_j X_{i:i+l-1} + b)) $.之后在用max-over-time pooling: $\tilde c_j=max{c_{j1},..., c_{jT-l+1}}$这里用的是不同大小的kernel来提取不同的特征,之后再将他们拼起来,得到$c_x$.同理,target language sequence可以得到$c_y$,最后,给定source language sequence,判断target language sequence是真的概率为$p=\sigma(V[c_x;c_y])$,V将输入转换为2维的embedding,$\sigma$是logistic函数.

**BLEU objective**

给定生成的$y_g$和ground truth $y_d$,可以计算奖励函数$Q(y_g,y_d)$,Q的取值是0到1.

**Policy gradient training**

![](/papers/tts/59.png)
其中R代表action-value函数.

![](/papers/tts/60.png)
其中,b表示baseline value,来减小reward的variance,实际上,可以将b设为常量0.5.

还有一个问题在于,给定source sequence,D通常只能在target sequence预测结束后给一个分数,因此我们不能获得一个实时的action-value.对于未知的token,这里采用了基于策略G的Monte Carlo search.采用N个采用的结果的平均值作为中间状态的reward.

一旦我们可以产生比较真实的输出,可以重新训练D:

![](/papers/tts/61.png)


更新完D之后,重新训练G:

![](/papers/tts/62.png)

首先用极大似然预训练G.之后用G产生语句,这里用greedy sampling而不是beam search.之后用true paralle data和machine产生的data来pretrain D,知道D的准确率可以达到某个值,这里用的0.82.最后,联合训练G和D,G用policy gradient来训练,不过实际上,发现用简单的policy gradient容易导致不稳定,因此这里用了teacher forcing.

**实验**

主要实验了$\lambda$和D的初始化准确率,采样个数对最终结果的影响.

----

#### Wasserstein GAN and WaveformLoss-based Acoustic Model Training forMulti-speaker Text-to-Speech SynthesisSystems Using a WaveNet Vocoder

利用了conditional GAN.

![](/papers/tts/63.png)

loss结合了MSE adversarial loss.

训练过程:

- generator预训练:用MSE训练
- adversarial training:用MSE和ADV训练
- finetune: 用wavenet loss对generator进行finetune.

----

#### Reg-Gan: Semi-Supervised Learning Based on Generative Adversarial Networks for Regression

目前也有一些GAN如improved-GAN,Cat-GAN,SGAN,Triple-GAN也是解决半监督问题,但是是用于分类问题的.这篇是目前第一篇半监督GAN用于回归问题的.

本文是基于Improved-GAN的改进.Improved-GAN的基本思想是用一个网络同时做分类和判别的作用,即输出N+1个值,前N个值进行分类,最后一个值则是代表True/False.除此之外,Improved-GAN用了feature matching来解决G的不稳定问题,传统的GAN的G努力的最大化D的输出,而这里则是最大化D中间generated的数据和真实数据的匹配程度,即最小化其差异.

这里提出了两种D的架构:

1. D有两种输出:第一种用来预测标签,第二种是预测real/fake.我们假设label可以正则化到[0, 1]范围,D用常见的非监督GAN loss和监督的regression loss:

![](/papers/tts/64.png)

z是噪声的分布,x是真实数据的分布,G(x)是生成数据的分布,y表示label的值,$\hat y$表示预测的label.不过这里的unsupervised loss实际用的是最小二乘loss.

2. 这里D只有一个输出:标签的预测值.之后将标签送入一个新的方程,基于预测的标签给input一个index.换句话说,这里没有用网络来区分真实数据和产生的数据,而是用了一个kernel function,接收regression output来判断预测出来的label是真是假.如果predicted label是在规划化的true label的范围内,比如在[0, 1]之内,那么给的index是1,否则给的index会是远小于1的值,根据相距的距离.

![](/papers/tts/65.png)

#### Generative adversarial text to image synthesis

从文本生成图片其实是多模态的,对于同一个文本,可以生成多种不同的图片.这篇论文设计了一种网络,包含有character-level encoder,和class-conditional GAN.这里的数据是$\{(v_n, t_n, y_n)\}$,其中v代表图片,t代表文本,y代表类别.


生成器接收噪音$z \in R^Z \sim N(0, 1)$,并且用text encoder对text进行编码.编码后的向量首先经过一层全连接压缩到一个低维度(128),之后是ReLU,之后拼在z上面.

在D中,进行了几层步长为2的卷积,结合spatial batch normalization和ReLU.同样的,也将text encoding经过全连接压缩.最后进行1x1的卷积+ReLU+4x4卷积. 卷积层都要用batch normalization.

在简单的GAN中,D接收两种输入: 真实的图片与与之对应的文本;合成的图片和任意的文本.因此,必须要区分开两种错误的来源: 不真实的图片(任意的文本)和真实的图片错误的类别.

这里我们进行了修改,将输入改成三种,添加了真实的图片和不匹配的文本,D应当输出fake.

另外这里对于G添加了一个新的loss:

$$E_{t_1,t_2\in p_{data}}[log(1-D(G(z, \beta t_1 + (1-\beta)t_2)))]$$

这里固定了β为0.5

如果text encoding可以捕捉图片信息,那么为了产生真实的图片,噪声z也应当包含有style factor比如背景噪声和pose.训练好GAN之后,我们希望可以将图片进行风格迁移.可以再构造一个style encoder network S, 优化squared loss即可.


-----

#### Languagea GANs Falling Short

这篇文章指出了目前language GAN普遍存在的问题.

本文的主题在于language GAN, 即将GAN用与NLG的任务. 目前NLG一种方法是MLE模型,即优化极大似然的模型,通常是用seq2seq结构,用teacher forcing的方式进行训练. 在不少论文中也提到过, 这种teacher forcing的方式会存在*exposure bias*的问题, 在inference的时候,decoder接收到的是上一时刻预测的值,而非训练时候给出的真值,这会带来累计误差. 因此有一种途径是通过GAN来做NLG任务,因为GAN并没有这种exposure bias问题. 这篇文章首先回顾了NLG的经典评估方式,指出目前quality-only evaluation的缺点:可以通过改变一个temperature parameter值来影响评估结果. 另外,本文还提出了通过更改这个参数来控制quality/diversity trade-off,结果发现MLE模型总是可以outperform所有提出的GAN的方法. 最后有两个结论:

- exposure bias没有想象的那么严重
- temperature tuning可以提供一种比对抗训练更好的quality/diversity trade off, 同时更加容易训练, 更容易交叉验证, 消耗计算更少


**introduction**



文本主要针对文本生成的问题, GAN用于文本生成目前也有不少文章, 如

1. Yu, L., Zhang, W., Wang, J., and Yu, Y. (2017).  Seqgan:Sequence generative adversarial nets with policy gradient.
2. Che, T., Li, Y., Zhang, R., Hjelm, R. D., Li, W., Song, Y.,and Bengio, Y. (2017). Maximum-likelihood augmenteddiscrete generative adversarial networks.arXiv preprintarXiv:1702.07983.
3. Lin, K., Li, D., He, X., Zhang, Z., and Sun, M.-T. (2017).Adversarial ranking for language generation. InAdvancesin Neural Information Processing Systems, pages 3155–3165.
4. Zhang, Y., Gan, Z., Fan, K., Chen, Z., Henao, R., Shen, D.,and Carin, L. (2017).  Adversarial feature matching fortext generation.arXiv preprint arXiv:1706.03850.
5. Guo, J., Lu, S., Cai, H., Zhang, W., Yu, Y., and Wang, J.(2017). Long text generation via adversarial training withleaked information.arXiv preprint arXiv:1709.08624.
6. Fedus, W., Goodfellow, I., and Dai, A. M. (2018). Maskgan:Better text generation via filling in the.arXiv preprintarXiv:1801.07736.
7. Lu,   S.,   Yu,   L.,   Zhang,   W.,   and   Yu,   Y.   (2018a).Cot: Cooperative  training  for  generative modeling.https://arxiv.org/pdf/1804.03782.pdf.
8. Shi, Z., Chen, X., Xipeng, Q., and Huang, X. (2018).  To-wards diverse text generation with inverse reinforcementlearning.https://arxiv.org/pdf/1804.11258.pdf
9. Xu,  J.,  Sun,  X.,  Ren,  X.,  Lin,  J.,  Wei,  B.,  and  Li,  W.(2018).  Dp-gan: Diversity-promoting generative adver-sarial network for generating informative and diversifiedtext.arXiv preprint arXiv:1802.01345.
10. Chen,  L.,  Dai,  S.,  Tao,  C.,  Shen,  D.,  Gan,  Z.,  Zhang,H., Zhang, Y., and Carin, L. (2018).   Adversarial textgeneration via feature-mover’s distance.arXiv preprintarXiv:1809.06297.

需要注意的是, 原始的GAN是用于连续数据的(图片), 而用于文本生成的话,则必须生成离散数据.

目前对生成模型标准的评估方法是计算model给held-out data算得的likelihood. 不同的模型可以用这种方式进行比较.

held-out likelihood比较通用, 但是它并不能针对所有的情况给出一个满意的解决途径. 首先, 从计算的角度来看, 在某些生成模型中这个东西比较难算, 另外这个likelihood并不能捕捉到所有必要的东西,在一些情况下,用针对任务的评估矩阵更加合适.


当然,还有一种评估方式是人为的评估模型产生的样本, 但这也是不切实际的.


最近提出的评估方式是在两个维度评估: 1) 每个样本的quality; 2) 样本之间的diversity. 下图左侧的图片是目前衡量的方法, 可以看到一种模型在quality上好一些,在diversity上差一些,这样就很难比较了. 本文指出, 模型的 *softmax temperature* 和 quality-diversity trade-off之间存在一定的联系: 低的 temperature 可以产生高质量的样本, 高的 temperature 可以产生diverse的样本(增加分布的熵), 控制temperature的过程, 本文称之为temperature sweep. 中间的图片表示红色的模型可以用于产生高diversity的样本, 蓝色的则可以用于产生高质量的样本. 右边的图则表示蓝色的模型优于红色的.

![](/papers/gans/1.png)


用这种方式衡量后, 发现MLE模型总是优于GAN模型.

**recent develoments in NLG**

GAN的目标函数可以定义为下式:

$$min_{\theta} max_{\phi} E_{x \sim p_{data}}[logD_{\phi}(x)] + E_{x \sim G_{\theta}}[1 - logD_{\phi}(x)]$$


GAN最初用于产生像图片那样的连续数据(0-1), 这是因为训练过程依赖于从D像G的后向传播. 离散型的数据需要其他的方法(无法传播), 如强化学习, 用强化学习的话, 判别器对所有的完整序列进行评估, 这样就需要进行k Monte-Carlo 采样知道序列停止. 还有一些其他的方式具体可以看上面列出来的几篇参考文献.

目前的这些GAN的方法都是用了sample  quality来做评估. 有一篇论文提出来将生成的一个句子同生成的多个句子进行比较, 称为self-BLEU, 提出了一种用Negative BLEU和self-BLEE的评估方法将quality和diversity都进行了比较.  但是呢, 按照他这个评估方法得到的结果确没有明确指出哪种模型更优, 如下图所示:


![](/papers/gans/2.png)


**temperature sweep: towards robust NLG evaluation**

NLG中可以利用Boltzmann temperature parameter $\alpha$来调节$G_{\theta}(x_t|x_{1:t-1}) = softmax(o_t \cdot W/\alpha)$

将$\alpha$设为大于1的数,会增大$o_t$, 从而减小G产生条件分布的熵. 这是NLG中减少错误概率,提高样本质量的一种有效手段. 这也就意味着, temperature tuning可以将模型在quality/diversity空间中移动.


 **other techniques to navigate in quality-diversity space**


 语言模型的联合概率分布通常是由条件概率分布的积得到的, 即:

 $$p(x_1, .., x_n) = \prod_1^n p(x_t|x_{t-1}, ..., x_1)$$

我们可以改变这些条件概率分布中的temperature, 但是这个和直接更改联合分布的temperature不一样. 这种方法产生的样本会有偏差, 相比联合分布更改temperature的方法,它会更倾向于在句子开始时候熵比较低. 下面会介绍两种方法.


1. stochastic beam search

在stochastic beam search中, 第一个时间步,采样k个单词, 在接下来的时间步中, 基于上一个单词再采样k个单词, 得到$k^2$个结果, 保留前k个. 更改k的值可以调节quality-diversity. 这种方式不会有偏差(为什么??). 不过和temperature tuning不同的是, bean size值是离散调节的. 

2. generator rejection sampling

生成一些语句, 利用Generator计算它们的likelihood, 给定一个阈值, 依次接收或者拒接这个样本. 这个阈值可以调节quality-diversity. 不过当阈值较低的时候, running time会显著增大.

**results**

实验包含两部分: synthetic data generation 和 long text generation.

1. synthetic data generation 

本文用的GAN基于seqGAN, 不过加了些其他东西: MLE pretraining, leaky discriminator, step level loss而不是sequence level loss, 学习baseline来减少方差, 用最大熵loss来正则化reinfor, 用MLE loss调节对抗loss. 判别器结构和生成器的一样, 这样可以用生成器利用MLE学来的权重初始化判别器.


用$Oracle  NLL_{oracle}$评估似然度.进行temperature sweep后评估的结果如下图所示, 可以看到MLE要优于GAN的方法, CoT与MLE比较接近, 这是因为他们两个的目标函数比较像.

![](/papers/gans/3.png)

2. long text generation


用negative BLEI和Self-BLEU来评估.

![](/papers/gans/4.png)


后面用新的评估方法,叫做language model score(quality)和reverse language model score(diversity+quality)来评估.


![](/papers/gans/5.png)

很多文章任务exposure bias会导致MLE效果差, 尤其是长句差, 但是实验表明MLE要优于GAN. 说明GAN的结合强化学习方法的优化问题更大.


另外,下图发现GAN在刚开始训练的时候, 熵急剧减小.
![](/papers/gans/6.png)


**结论**

本文发现temperature sweeping可以更改模型的表现. 调好的language model优于GAN的model


------

#### Disentangling Correlated Speaker and Noise forSpeech Synthesis via Data Augmentation andAdversarial Factorization

NIPS2018 paper



