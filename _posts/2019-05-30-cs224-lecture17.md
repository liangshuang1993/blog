#### Multitask Learning

这节课主要介绍了multitask learning, 主讲是这篇文章的作者.

#### The Natural Language Decathlon: Multitask Learning as Question Answering


这篇文章提出了decaNLP, 完成question answering, machine translation, summarization, natural language inference, sentiment analysis, semantic role labeling, relation extraction, goal-oriented dialogue, semantic parsing, commonsense pronoun resolution十种任务. decaNLP希望用一个模型同时优化这十种任务,具有更好的泛化性.


通过允许任务规范采用自然语言问题q的形式, 将将所有的task构建成question answering, 所有的输入具有一个context, question和answer. 如下图所示, 可以将不同的任务转成QA的形式.

![](/papers/tts/73.png)

传统的NLP任务有x作为输入, y作为输出, 任务t则是通过限制模型给定的. 而meta-learning则是将t也作为了额外的输入. 本文的方法中并没有对t做表征,而是用了natural language questions来提供任务的描述.从而实现multitask learning, 使得他们更适合作为pretrained model来进行transfer learning和meta-leaning. 

decaNLP针对每个任务用了相应的数据集,具体可以看论文. 



**Multitask Question Answer Network**

十种task都是以question answering的形式进行训练的,所以这里的模型采用的也是一种multitask question answering network的结构, 每个样本包含一个context, question和一个answer. 最近很多QA模型假设answer可以直接从context中复制过来, 但是这种假设在更加general的qa问题中不适用. 问题通常包含着能限制答案空间的关键信息, 因此本文采用了coattention来同时丰富input和question的表征. 同时还用了pointer-mechanism, 这两种机制都是参考的两篇相应论文.

context c 有 l 个token, question q 有 m 个token, answer a 有 n 个 token. 这些都用矩阵来表示, $C \in R^{l\times d_{emb}}, Q \in R^{m\times d_{emb}}, A \in R^{n\times d_{emb}}$. 一个编码器接收这些矩阵作为输入,编码器中有一些recurrent, conattentive, 自注意力层,之后产生context 和 question序列的最终表征 $C_{fin} \in R^{l \times d}, Q_{fin} \in R^{m \times d}$. 解码器首先将answer embedding投影到d维空间上, $AW_2=A_{proj} \in R^{n \times d}$, 之后有一个自注意力层, 另外和transformer一样, 也需要加入位置编码. PE[t, k]. encoder-decoder之间的attention用的是多头注意力. 解码器中用LSTM来得到context vector, 即

$$LSTM([{(A_{self})}_{t-1};\tilde c_{t-1}], h_{t-1}) = h_t \in R^d$$

用decoder的隐状态来获取attention weights,

$$softmax(C_{fin}(W_2h_t)) = \alpha^C_t \in R^l$$     
$$softmax(C_{fin}(W_3h_t)) = \alpha^Q_t \in R^m$$


之后可以得到context vector:

$$tanh(W_4[C_{fin}^T\alpha_t^C;h_t]) = \tilde c_t \in R^d$$
$$tanh(W_5[C_{fin}^T\alpha_t^Q;h_t]) = \tilde q_t \in R^d$$

multi-pointer-generator:

模型应当可以产生不在context或者question中的单词, 我们给它获取v个额外的单词的权限,

$$\sum_{i:c_i=w_t}(\alpha^C_t)_i = p_c(w_t) \in R^n$$
$$\sum_{i:q_i=w_t}(\alpha^Q_t)_i = p_q(w_t) \in R^n$$