### CTC原理介绍

以下内容主要来自 https://www.cnblogs.com/qcloud1001/p/9041218.html

另有博客 https://xiaodu.io/ctc-explained/ 介绍得非常清楚。


目前主流的语音识别大都分为特征提取，声学模型，语音模型几个部分。而目前结合神经网络的端到端的声学模型训练方法主要是CTC和基于attention两种。

CTC算法全程叫做：Connectionist temporal classification。解决时序类数据的分类问题。

传统的语音识别的声学模型，对于每一帧的数据，需要知道对应的label才能进行有效的训练，在训练数据之前需要做语音对齐的预处理，而语音对齐的过程本身就需要进行反复多次的迭代，来保证对齐更准确，非常耗时。对于每一帧音频，需要知道其对应的哪个发音因素，如第1,2,3,4帧对应n的因素，第5,6,7帧对应i的因素,第8,9帧对应h的因素等等。

与传统的声学模型训练相比，采用CTC作为损失函数的声学模型训练，是一种端到端的训练方式，不需要预先对数据做对齐，只需要一个输入序列和一个输出序列即可训练，这样就就不需要对数据对齐和一一标注，并且CTC直接输出序列预测的概率，不需要外部的后处理。

CTC是一个输入序列到另一个输入序列，因此它只关心输出序列是否和真是序列接近，而不关心输出序列中每个结果在时间点上是否和输入的序列正好对齐。

CTC引入了blank(该帧没有预测值)，每个预测的分类对应一整段语音中的一个spike(尖峰),其他不是尖峰的位置认为是blank。CTC最后输出是spike的序列，并不关心每一个音素持续了多长时间。

CTC也可以用于OCR，比如之前做的CRNN其实也是用了CTC。

训练集合为$S=\{(x^1,z^1),(x^2,z^2),...,(x^N,z^N)\}$ 表示有N个训练样本，x是输入样本序列，z是对应的真是输出的label序列。对于语音识别来说，输入的序列长度大于输出的序列长度。

对于其中的一个样本$(x,z),x=(x_1,x_2,x_3,...x_T)$表示一个长度为T帧的数据，每一帧的数据是一个维度为m的向量，$x_i$可以理解为对于一段语音，每25ms作为一帧，其中第i帧的数据经过MFCC计算后得到的结果。

$z=(z_1,z_2,...z_U)$表示这段样本语音对应的正确的因素，如z=[n,i,h,a,o] (这里暂时将拼音的字母当做一个因素)

特征x在经过RNN的计算之后，再经过一个softmax层，得到因素的后验概率y。$y^t_k(k=1,2,3,...n,t=1,2,3,...,T)$表示t时刻发音为音素k的概率，其中音素的种类个数一共有n个，在每一帧的数据上所有的音素概率加起来为1.这个过程可以看成是对输入的特征数据x做了变换$N_w:(R^m)^T->(R^n)^T$，如下图所示，最下面的图即表示各个时刻各音素的概率矩阵。



#### 路径π和B变换


由于实际训练中并不知道每一帧对应的音素，因此训练比较困难。先考虑一种简单的情况，已知每一帧的因素的标签$z'$，即训练样本为$(x,z')$，其中$z'$不是简单的[n,i,h,a,o]，而是

![](/papers/tts/28.png)
其中$T_1+T_2+T_3+T_4+T_5=T$

在这种情况下有$P(z'|x)=p(z'|y=N_w(x))=y^1_{z'_1}y^2_{z'_2}...y^T_{z'_T}$即后验概率的乘积。

我们希望乘积越大越好，因此数学上可以写出

$min_w-log(y^1_{z'_1}y^2_{z'_2}...y^T_{z'_T}), subject to: y=N_w(x)$

目标函数对于后验概率矩阵y中的每个元素$y^t_k$的偏导数为：

![](/papers/tts/29.png)

也就是说，在每个时刻t(对应句很的一列)，目标只与$y^t_{z'_t}$相关的。在这个例子中是与被框起来的元素相关。

其中$N_w$可以看做是RNN模型，如果训练数据的每一帧都标记了正确的音素，那么训练过程就很简单了，但是实际上这样的标记过的数据非常稀少，CTC可以做到用未逐帧标记的数据做训练。


首先定义几个符号：
$L=[a,o,e,u,i,...]$表示所有的音素的集合
$\pi=(\pi_1,\pi_2,...\pi_T), \pi_i \in L$, 表示一条由L中元素组成的长度为T的路径,比如$z'$就是一条路径。

定义B变换，表示简单的压缩，例如：
$B(a,a,a,b,b,b,c,c,d=(a,b,c,d))$

一个路径$\pi$经过B压缩之后只要满足真实的音素，即可认为它是在说这些音素。

路径$\pi=(\pi_1,\pi_2,...\pi_T)$的概率为它所经过的矩阵y上的元素相乘：
$P(\pi|x)=P(\pi|y=N_w(x))=P(\pi|y)=\prod^T_{t=1}y^t_{\pi_t}$

因此，在没有对齐的情况下，目标函数应为$\{\pi|B(\pi)=z\}$中所有元素概率之和。即
$max_wP(z|y=N_w(x))=P(z|x)=\sum_{B(\pi)=z}P(\pi|x)$

在T=30，音素为[n,i,h,a,o]的情况下，共有$C^5_29$条路径可以被压缩为[n,i,h,a,o]，量级大约为$(T-1)^{音素个数}$,显然这么大的路径数目是无法直接计算的，因此CTC方法中借用了HMM中的前向后向算法来计算。

#### 训练实施方法

CTC的训练过程是通过$\frac{dP(z|x)}{dw}$，调整w的值使得上面式子的目标值最大，而计算的过程如下：

![](/papers/tts/31.png)

下面以你好为例，介绍该值的计算方法：

首先找出所有可能被压缩为$z=[n,i,h,a,o]$的路径，记为$\{\pi|B(\pi)=z\}$。所有的π都有[n,n,n,....,n,i,.....,i,h,.....h,a,....a,o,...,o]的形式，即目标函数只与后验概率矩阵y中表示n,i,h,a,o的5行相关，因此为了简便，我们将这5行提取出来，如下图所示。

![](/papers/tts/32.png)

在每一个点上，路径只能向右或者向下转移，红色和绿色为两条路径，分别用q和r表示，这两条路径都经过$y^{14}_h$这点，表示这两点路径均在第14帧的时候在发h这个音。因为在上面目标函数的连加项中，有的项与$y^{14}_h$无关，因此可以剔除这一部分，只留下与之有关的部分，记为$\{\pi|B(\pi)=z,\pi_{14}=h\}$

![](/papers/tts/33.png)

可以发先从q和r可以推出四条路径都是可行的路径。

![](/papers/tts/34.png)

可以发现，该值可以总结为：(前置项)$\cdot y^{14}_h\cdot$(后置项)。因此对于所有经过$y^{14}_h$的路径，有

![](/papers/tts/35.png)

定义：
$\alpha_{14}(h)=(前置项).y^{14}_h = \sum_{B(\pi_{1:14})=[n,i,h] }\prod_{t\prime=1}^{14} y^{t\prime}_{\pi_{t\prime}} $
该值可以理解为从初始到$y^{14}_h$这一段，所有正向路径的概率之后。

另外，可以发现$\alpha_{14}(h)$可以由$\alpha_{13}(h)$和$\alpha_{14}(i)$递推得到：

$\alpha_{14}(h)=(\alpha_{13}(h)+\alpha_{13}(i))y^{14}_h$

这个公式的含义是，只有t=13时发音是h或者i，在t=14时才有可能是发音h.那么在t=14时刻发音是h的所有正向路径概率$\alpha_{14}(h)$就等于是在t=13时刻，发音为h的正向概率$\alpha_{13}(h)$加上发音为i的正向概率$\alpha_{13}(i)$，再乘以当前音素被判断为h的概率$y^{14}_h$，由此可知，每个$\alpha_t(s)$都可以由$\alpha_{t-1}(s)$和$\alpha_{5-1}(s-1)$两个值得到。

![](/papers/tts/36.png)

即每个值都由上一时刻的一个或者两个值得到，类似定义

$\beta_{14}(h)=(\beta_{15}(h)+\beta_{15}(a))y_h^{14}$

因此有：

![](/papers/tts/37.png)

得到此值之后，就可以根据反向传播算法进行计算了。计算$\alpha, \beta$的计算量大约各为2T(音素个数)，之后再计算对每个$y^t_k$的偏导值的计算量为3T(音素个数)，总的计算量为7T(音素个数)，非常小。

