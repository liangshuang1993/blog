#### Contextual word embedding

tips for unknown word with word vectors:

- use character-level model
- if the <UNK> word at test time appears in your unsupervised word embeddings, use that vector


word vector存在的问题:
- 每个word的含义可能有很多种,但是只有一个embedding,需要知道context中的含义
- 每个单词只有一个vector,但是word可能有多个维度,比如不同的含义,不同的时态,不同的词性,

采用semi-supervised方法.

![](/courses/cs224/images/lecture13/1.png)

如上图所示,可以先用无标签数据训练一个word embedding如word2vec,同时也训练一个language model,如用BiLSTM训练;后面冻结这个网络.

![](/courses/cs224/images/lecture13/2.png)

假设这里的task是name entity.将一句话送入language model,得到embedding.另外,还有一个name entity网络,接收word embedding(character level),第二步就是训练name entity网络.

Elmo就是一个contextualized word representation.(https://allennlp.org/elmo)

Elmo和TagLM不同的地方(不过分的增大网络来提升性能):
- 用两层BiLSTM
- 用character CNN创建最初的word representation, 2048 char n-gram filters,2层highway,512 dim projection
- 4096 hidden/cell LSTM states,投影到512维作为下一时刻的输入
- 用residual connection

TagLM送入后面网络的是language model的top level的输出,而Elmo用BiLSTM的所有输出的加权和.

两层BiLSTM其实获取的是不同的信息:
- low layer更适合获取low-level syntax. 适用于part-of-speech tagging, syntactic dependencies, NER
- high layer更适合high-level semantics,适用于sentiment,semantic role labeling, question answering, SNLI

ULMfit: text classification

- 在general domain 上训练一个LM
- 在target task data上 tune LM
- 在target task上 作为一个分类器finetune(两个任务)


后面介绍了transformer和Bert,具体原理可以看attention和nlp-papers两篇文章
