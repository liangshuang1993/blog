#### Natural Language Generation

首先回顾以下language modeling的定义:给定已有的单词,预测下一个单词的概率.

$$P(y_t|y_1,...y_{t-1})$$

产生这种分布的模型就叫做language model.

conditional language modeling: 除了给定已有的单词,还有了其他的输入
$$P(y_t|y_1,...y_{t-1}, x)$$

如:
- machine translation(x=source sentence, y=target sentence)
- summarization(x=input text, y=summarized text)
- dialogue(x=dialogue history, y=next utterance)
- ...

再来回顾以下beam search decoding:

k比较小的时候,容易造成greedy decoding:
- ungrammatical, unnatural, nonsensical, incorrect

k比较大的时候,计算量更大:
- 太大的k会减小BLEU得分,可能是因为大的k容易产生短句
- 在open-ended task,如chit-chat dialogue中,大的k可能产生的输出更加generic,如下图所示:

![](/courses/cs224/images/lecture15/1.png)

还有一种方式是sampling-based decoding

- pure sampling: 每个时间步t,从输出的分布P进行随机采样
- top-n sampling: 也是从P进行采样,但是只保留概率最大的n个单词,n=1即是greedy search,n=V时是pure sampling;n较大时产生的输出更加多样化



softmax temperature:

可以对输出的softmax增加一个参数$\tau$

![](/courses/cs224/images/lecture15/2.png){:height="100px" width="400px"}

- 增大$\tau$, p趋近于均匀分布,产生的输出更加多样化
- 减小$\tau$,p更加不均匀,产生的输出会聚集在概率较大的单词上

softmax temperature不是decoding算法,它只需要在测试的时候使用来改变decoding的输出,可以和decoding算法结合使用.




-----

#### NLG tasks and neural approaches to them


##### 以summarization为例,

task: 给定input text x, 写出一段输出y,包含x的主要部分且比x短

- single-document: 写一个文件x的summarization
- multi-document: 从多个文件写出一个y,通常多个文件中含有共同部分.

summarization的评估标准常用ROUGE(Recall-Oriented Understudy for Gisting Evaluation)

和BLEU相似,它是基于n-gram overlap的,区别在于:
- ROUGE没有短语句惩罚
- 基于recall,而BLEU基于precision.(precision对于MT更重要,recall对于summarization更重要)
- BLEU是一个single number,是n=1,2,3,4 n-gram的结合, 而ROUGE是每个n-gram都对应一个值,ROUGE-1, ROUGE-2, ROUGE-L


1. copy mechanisms

- seq2seq+attention可以产生流畅的输出,但是不擅长正确地copy细节
- copy mechanism用attention来使得seq2seq更容易从输入copy words and phrases到输出
- copy mechanism存在的问题是copy的太多了,并且不擅长整体的content的选择,尤其是input很长的时候

2. 可以采用bottom-up summarization
- content selection stage: 用neural sequence-tagging model,输出的tag是include或者don't include
- bottom-up attention stage: 用seq2seq+attention模型,但是并不会注意到don't include的单词

3. 用强化学习

用强化学习直接优化ROUGE-L(极大似然不能直接优化ROUGE-L,因为它不可微)

但是用RL,会产生高的ROUGE-L,但是human judgement score分数会更低.可以用结合的方法


##### dialogue

seq2seq经常产生用户语句无关的语句
一种可以解决的途径是更改优化函数,由原先的p(T|S)改为,S和T之间的互信息(maximum mutual information)

$$log \frac {p(S, T)}{p(S)p(T)}$$
$$\hat T=argmax_T\{logp(T|S)-logp(T)\}$$

如何修复generic/boring response problem:

- easy test-time fixes:
    - 增大 beam search中rare word的权重
    - 用sampling decoding而不是beam search
    - softmax temperature
- conditional fixes:
    - 加入以下额外的condition
    - 训练一个retrieve-and-refine model而不是generate-from-scratch model


repetition problem:
- beam search中,如果会造成n-gram repeating,直接扔掉
- 训练一个converage mechanism, 防止attention多次注意到一个单词
- 定义一个阻止repetition的目标函数,如果函数不可微,可能需要其他技术,如RL来训练


##### storytelling

这里举了个例子,如何用Taylor Swift的风格为图片写故事.

训练过程如下:

- 用COCO(images captioning dataset),学习从image到skip-thought encoding of captions的映射
- 从目标风格预料(Taylor Swift歌词),训练一个RNN-LM来对skip-thought vector进行解码
- 结合前两者


目前NLG还没有一个很好的评估标准
