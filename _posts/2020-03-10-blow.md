Blow 是一个基于glow的，many to many的VC模型，它是直接生成的raw audio。 NIPS2019


Blow相比较Glow的结构具有以下改进：

- single-scale structure
- 每个block中有一个flow，即更多的block，block中的flow个数更少
- forward-backward conversion mechanism
- 基于hypernetworks的condition module
- 共享的speaker embedding
- data augmentation

这些改进可以产生更高的likelihood，而且可以提升效率


目前flow-based generative model最流行的应该是RealNVP和Glow，不过最近也有很多新的算法提出来了，比如
- FFJORD
- Flow++
- A RAD approach to deep mixture
models
- TzK Flow - Conditional Generative Model
- Conditional recurrent flow: conditional generation of longitudinal
samples with applications to neuroimaging
- Emerging convolutions for generative
normalizing flows

这些都是2018和2019年发表的论文，后面要仔细读下。


#### Blow的结构



![](/papers/flow/3.png)


**single-scale structure**

文章发现speaker identity 特征只在比较粗的level上面保存，并且，移除掉multi-scale之后gradient更不容易出问题，同时获得的log-likelihood更加高。

**many blocks**

图片分辨率通常是32x32, 256x256, 对于audio来说，16KHz下的256个样本对应的是16ms，但对捕捉语音中有用的信息可能是不够的。phoneme duration通常是50到180ms。所以我们需要增大感受野。WaveGlow用了空洞卷积来增大感受野，但是在Blow中，采用的方法是用更多的block，以及每个block中更少的flow step。采用了8个block，每个block中12个flow step。每个block都有一个2x 的squeeze operation， 所以最终squeezing的是256个样本. 

8x12的结构，两层kernel size是3的卷积，对应的感受野大概是12500（12800？）个样本，大概是781ms。为了让batch size更大，这里采用的input是4096个样本，这个已经足够了。这里提到Blow是在一帧一帧地运行，没有context，这可能会不能很好的建模long-range speaker-dependent prosody，不过应该足够建模speaker identity。


**forward-backward conversion**


Glow-based模型在image manipulation和class-conditioning方面常用的策略是在z space做文章。但是本文发现，在VC领域，这种策略不是很有用。这篇文章引入了另一个condition，即speaker identity。

**hyperconditioning**

flow-based model引入condition方式是coupling network，很大的计算都是在coupling network中完成的，它需要具有很强的转换能力。一种常见的coupling network是add或者concat到input上面。但是，concatenation常常被忽视，add也不够强大，这里将condition直接作用在CNN的权重上，像hypernetwork一样。这里在coupling network的第一层实现。

定义CNN为：

$$h^{(i)} = W_y^{(i)} * H + b_y^{(i)}$$

$$K_y = {(W_y^{(1)}, b_y^{(1)}) ... (W_y^{(n)}, b_y^{(n)})} = g(e_y)$$

$e_g$是condition，g是adapter network。


**shared embedding**

每个coupling network学习一个$e_g$通常会产生局部最优解。这里采用一个learnable embedding $e_g$，可以减少参数量，且结果更好。同样地，adapter network也很小，一个简单的线性层即可。

**data augmentation**

- temporal jitter
- pre-/de-emphasis filter
- random amplitude scaling
- randomly flip the values in the frame

这篇文章开源了代码，所以可以对着代码看一遍。


它的1x1可逆卷积，即channel mixer部分比较独特

```python
class InvConv(torch.nn.Module):

    def __init__(self,in_channel):
        super(InvConv,self).__init__()

        weight=np.random.randn(in_channel,in_channel)
        q,_=linalg.qr(weight)
        w_p,w_l,w_u=linalg.lu(q.astype(np.float32))
        w_s=np.diag(w_u)
        w_u=np.triu(w_u,1)
        u_mask=np.triu(np.ones_like(w_u),1)
        l_mask=u_mask.T

        self.register_buffer('w_p',torch.from_numpy(w_p))
        self.register_buffer('u_mask',torch.from_numpy(u_mask))
        self.register_buffer('l_mask',torch.from_numpy(l_mask))
        self.register_buffer('l_eye',torch.eye(l_mask.shape[0]))
        self.register_buffer('s_sign',torch.sign(torch.from_numpy(w_s)))
        self.w_l=torch.nn.Parameter(torch.from_numpy(w_l))
        self.w_s=torch.nn.Parameter(torch.log(1e-7+torch.abs(torch.from_numpy(w_s))))
        self.w_u=torch.nn.Parameter(torch.from_numpy(w_u))

        self.weight=None
        self.invweight=None

        return

    def calc_weight(self):
        weight=(
            self.w_p
            @ (self.w_l*self.l_mask+self.l_eye)
            @ (self.w_u*self.u_mask+torch.diag(self.s_sign*(torch.exp(self.w_s))))
        )
        return weight

    def forward(self,h):
        if self.weight is None:
            weight=self.calc_weight()
        else:
            weight=self.weight
        h=torch.nn.functional.conv1d(h,weight.unsqueeze(2))
        logdet=self.w_s.sum()*h.size(2)
        return h,logdet

    def reverse(self,h):
        if self.invweight is None:
            invweight=self.calc_weight().inverse()
        else:
            invweight=self.invweight
        h=torch.nn.functional.conv1d(h,invweight.unsqueeze(2))
        return h
```


pytorch中 **@**是用来进行矩阵相乘的。