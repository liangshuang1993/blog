#### Coreference Resolution


1. what is coreference resolution?

- 中文名字叫做指代消解. 指出句中表示同一entity的地方. 
- 比如 Barack Obama travaled to ... Obama.

有一个比较容易混淆的概念叫做anaphora(回指): 
- anaphor本身没有含义,但它由前面的antecedent的含义所决定. 
- Barack Obama(antecedent) said he(anaphor) would sign the bill.
- We want to see *a concert* last night. *The tickets* were really expensive.

![](/courses/cs224/images/lecture16/1.png)

另外还有一个概念叫做cataphora, 由后面的词的含义所决定．不过目前不再用这个名词了,用了anaphora替代.


![](/courses/cs224/images/lecture16/2.png)


-----
这节课介绍了四种coreference models

**传统代词消解方法 Hobbs' naive algorithm**

Hobbs手写的规则集, 在句法树上运作. 有两页的规则, 根据英文语言上的直觉编写, 可以得到80%的准确率, 有时候也可以作为其他机器学习分类器的特征之一使用.


**knowledge-based pronominal coreference**

> She poured water from **the pitcher** into **the cup** until **it** was full.
> She poured water from **the pitcher** into **the cup** until **it** was empty.

上面两句话的结构相同, 因此用Hobbs' algorithm无法得到正确答案,这时候就需要一个知识库了.


**Mention Pair**

将所有的指代词与所有被指代的词当成一个pair, 对每个pair进行二分类, 判断他们是否coreference.


另外,如果A和B是coreference的, B和C是coreference, 那么可以说A和C是coreference的. 所以可以做一个transitive closure, 最后可以得到一些clustering. 不过这里的操作比较危险, 如果多了一个线, 可能会导致所有的词都被视作coreference.


**mention ranking**

直接对指定的Mention查询其他的概率, softmax多分类. 优化函数为:


$$\sum_{j=1}^{i-1}1(y_{ij}=1)p(m_j, m_i)$$


有两种构建coref model的方法:

- non-neural coref models: Features
- neural coref modes: word embeddings and a few categorical features(传统方法里面的feature)

![](/courses/cs224/images/lecture16/3.png)



**end-to-end model**

2017年的模型

![](/courses/cs224/images/lecture16/4.png)

![](/courses/cs224/images/lecture16/5.png)

目前存在的问题:

- O(T^2) spans of text in a document(word number 是T)
- O(T^4) runtime
- 需要进行很多的剪枝工作.

**clustering-based**

每个mention是一个cluster, 之后做决定,是否要merge两个clusters. 这种方法可以先将简单的coreference找出来.


![](/courses/cs224/images/lecture16/6.png)



**coreference evaluation**

目前有很多: MUC, CEAF, LEA, B-CUBED, BLANC, 经常采用多个metrics的均值.

![](/courses/cs224/images/lecture16/7.png)




