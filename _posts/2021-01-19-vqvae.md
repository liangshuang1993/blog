### Neural Discrete Representation Learning


#### Introduction

目前有一些challenging tasks,比如few-shot learning, domain adaptation, reinforcement learning严重依赖于从raw data中学习representations。但是目前unsupervised学习representation的方式跟主流方法的差距还很大。目前无监督方法在pixel领域常用的loss有maximum likelihood和reconstruction error，然而，这两种loss的效果都和所使用的领域有关。这篇文章的目标是优化maximum likelihood来保留重要的特征，先学习discrete latent variables，之后可以使用强大的autoregressive model，比如pixelCNN基于这些variables来进行建模。这篇文章的贡献有：

- 介绍了VQ-VAE，是一个简单的，利用离散的隐变量，同时不会suffer from "posterios collapse",没有variance问题
- 表明VQ-VAE模型在log-likelihood上面和连续模型表现一样好
- 表明不需要supervision，可以从原始音频中学习language，另外，可以进行unsupervised speaker conversion


#### VQ-VAE

VAE包括以下几个部件：一个encoder建模后验分布$q(z|x)$，一个先验分布$p(z)$，一个decoder建模输入数据 $p(x|z)$

通常，都会假设VAE中的后验分布和先验分布都是一个正态分布，协方差为对角阵，之后利用重参原则进行优化。这篇文章则是将后验和先验分布都设置为离散的几个类别，从这些分布中采样是从一个embedding table中按索引取值。之后embedding被作为decoder网络的输入。


定义一个latent embedding space $e \in R^{K \times D}$

首先用encoder来从$x$中提取一个$z_e(x)$，之后在embedding space $e$中用最大近邻法查找discrete latent variable $z$，公式如下

$$
q(z=k|x) = \left\{ 
\begin{aligned}
x & =  1~~for~k=argmin_j||z_e(x) - e_j||_2 \\
z & =  0~~otherwise
\end{aligned}
\right.
$$

decoder的输入就用下式计算出的相应的$e_k$:

$$
z_q(x) = e_k,~where~k=argmin_j||z_e(x)-e_j||_2
$$



![](/img/posts/10.png)

需要注意的是，公式2是不能求梯度的，不过我们可以做一个近似，利用Straight-Through Estimator的方法，直接将decoder输入地方的梯度拷贝到encoder输出的地方。也可以利用量化操作求一些subgradient，但是上面这个近似效果就挺好的了。

Straight-Through的思想很简单，就是前向传播的时候可以用想要的变量（哪怕不可导），而反向传播的时候，用你自己为它所设计的梯度。根据这个思想，我们设计的目标函数是：

$$
‖x−decoder(z+sg[z_q−z])‖_2^2
$$

sg是stop gradient的意思。前向计算的时候，等价于最小化$decoder(z_q)$,求梯度的时候，sg部分不算梯度，等价于decoder(z)，这个就允许我们对encoder进行优化了。

另外，由于我们是根据最近邻搜索得到$z_q$的，所以我们其实希望$z_q$和$z$尽可能地相近，这样做近似的时候误差才小，我们可以在loss中增加一项：

$$
‖z−zq‖_2^2
$$

我们将上式分解为:

$$
‖sg[z] - z_q‖_2^2 + \beta ‖sz - sg[z_q]‖_2^2
$$

论文中支出$\beta$从0.1到2都可以，它采用了0.25


前面已经提到过，VQ-VAE可以用较少的隐变量+一个强大的自回归模型来建模，ImageNet试验中，隐变量为32x32个，CIFAR10中用了8x8x10。loss需要求平均。


实验部分只介绍下Audio

数据集： VCTK，109说话人

encoder： 6 strided convolutions with stride 2 and window-size 4

latent：比原始数据小了64倍，维度是512

decoder: condition是latent和one-hot speaker embedding

实验表明，生成出来的音频和原始音频的content （text context）一致，但是波形很不一样而且韵律也发生了变化，这表示VQ-VAE在没有linguistic feature的情况下，可以无监督地学习一些high-level的信息，

这篇文章还做了个VC的实验，也表示了VQ-VAE可以取出speaker相关的信息，the embeddings not only have the same meaning regardless of details in the waveform, but also across different voice-characteristics.

最后，还有一个分类任务

latent是128维，将每个128维的latent投射到41个可能的phoneme上面，准确率是49.3%（随机的latent space是7%）
