how to find an interesting place to start?
- look at ACL anthology for NLP papers:
--https://aclanthology.info
- online proceedings of major ML conferences:
--NeurIPS, ICML, ICLR
- cs224n project
--class website
- look at online preprint servers
--https://arxiv.org
- look for an interesting problem in the world
- http://www.arxiv-sanity.com/


如何获取data
- kaggle
- research papers
- https://machinelearningmastery.com/datasets-natural-language-processing/
- https://github.com/niderhoff/nlp-datasets


后面回顾了之前讲的内容，gradient vanish，GRU，LSTM...

RNN层最后的softmax其实占据了很大的工作，因此，为了减少计算量，通常限制一个固定大小的vocabulary，如50K。但是这就会导致UNK的出现：
- 如果是source sequence有UNK的话，没有很大影响，仍是给一个相应的word vector。
- 如果是target sequence有UNK的话，会有几种解决方法：
    - hierarchical softmax： tree-structured vocabulary，更快地做softmax
    - noise-contrastive estimation：binary classification，使得训练更快。
    - train on a subset of the vocabulary at a time, test on a smart on the set of possible translation.
    - use attention to work out what you are translating
    - more: word pieces,char.models

MT evaluation