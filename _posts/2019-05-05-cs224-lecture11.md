### Convolutional Networks for NLP


这节课老师推荐了一本书*Natural Language Processing with PyTorch*

前面讲到的模型通常都是使用了RNN，但是RNN也存在一些问题，比如:
- cannot capture phrases without prefix context.
- often capture too much of last words in final vector

因此，这里就引入了CNN，直观来讲:
> what if we compute vectors for every possible word subsequence of a certain length?

卷积的概念可以参考cs231的内容，本文就不介绍了。
在文本的处理上，通常用的是1D卷积：

![](courses/cs224/images/lecture11/1.png)

和图像相似，这里的卷积也可以使用padding，可以有多个channel，可以有pooling

![](courses/cs224/images/lecture11/2.png)

PyTorch实现起来也很简单：

```python
batch_size = 16
word_embed_size = 4
seq_len = 7
input = torch.randn(batch_size, word_embed_size, seq_len)
conv1 = Conv1d(in_channels=word_embed_size, out_channels=3, kernel_size=3) # can add: padding=1
hidden1 = conv1(input)
hidden2 = torch.max(hidden1, dim=2) # max pool
```

也可以使用空洞卷积(dilation)，可以增大感知野
![](https://pic2.zhimg.com/50/v2-4959201e816888c6648f2e78cccfd253_hd.gif)

__Single Layer CNN for Sentence Classification__

- word vector: $x_i \in R^k$
- sentence: $x_{1:n}=x_1\oplus x_2 \oplus ... \oplus x_n$  (vector concate)
- concatenatation of words in range: $x_{}i:i+j$ (symmetric more common)
- convolutional filter: $w \in R^{hk}$ (over window of h words)。kernel size是2,3,4
- result is a feature map
- 之后对feature map做pooling

另外他还用了一个trick

- 采用了pretrained vector(word2vec or Glove)，用两份拷贝数据
- 一份数据保持不变，另一份做finetune
- 两份数据在池化层之前加入feature map

模型整体结构如下图所示：
![](/courses/cs224/images/lecture11/3.png)

- dropout, create masking vector r of Bernoulli random variables with probability p of being 1
- delete features during training
- at test time, no dropout, scale final vector by p
- constrain L2 norm of weight vectors of each class to fix number s, not very common. not sure if this is neccessary.

对于比较深的网络，和LSTM与GRU的思想类似，可以添加直接连接的层，不过这里是垂直方向的，其实就是resnet

![](/courses/cs224/images/lecture11/4.png)

- 需要注意这里可能需要padding，才能保证加法的两边维度相同。

还有一个比较重要的点是batch normalization

- CNN中非常常见
- 对每一个batch中卷积的输出做变换，减去均值，除以方差，即得到均值为0，方差为1的向量。
- 采用batch normalization使得模型对参数初始化没有那么敏感。也会使得对于learning rate的调节更加简单
- pytorch中： nn.BatchNorm1d

1x1 convolutions, kersize_size = 1
- 起的作用类似于一个全连接网络，不过参数少得多
- 可以用来做channel上的变化。

CNN application: translation

- use CNN for encoding and RNN for decoding

 convolution over characters

