### 各种attention结构，以及相应代码

在早期的机器翻译应用中，神经网络一般是如下图的Seq2seq结构，左边是encoder，右边是decoder，encoder会通过RNN将最后一个step的隐藏状态向量c作为输出，如下图所示。

![](/papers/tts/24.png)

![](/papers/tts/25.png)

记encoder hidden states为$$E=mc^2$$ is a inline formula
\\(123)\\

$$
R_{\mu \nu} - {1 \over 2}g_{\mu \nu}\,R + g_{\mu \nu} \Lambda
= {8 \pi G \over c^4} T_{\mu \nu}
$$
